<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Neural Network Accelerators (2024) | Q. Wach</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Neural Network Accelerators (2024)" />
<meta name="author" content="Quentin Wach" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An short overview of mostly commercial AI hardware. A rough comparison of ASICs, FPGAs, GPUs, and looking at compute requirements and energy consumption. I end this post with the realization that we need to take a first-principles physics approach to narrow down what future AI accelerator designs will have to look like because the market is complex and largely driven by momentum." />
<meta property="og:description" content="An short overview of mostly commercial AI hardware. A rough comparison of ASICs, FPGAs, GPUs, and looking at compute requirements and energy consumption. I end this post with the realization that we need to take a first-principles physics approach to narrow down what future AI accelerator designs will have to look like because the market is complex and largely driven by momentum." />
<link rel="canonical" href="http://localhost:4000/personal/2024/04/10/AI-chip-market.html" />
<meta property="og:url" content="http://localhost:4000/personal/2024/04/10/AI-chip-market.html" />
<meta property="og:site_name" content="Q. Wach" />
<meta property="og:image" content="http://localhost:4000/images/AI_acc_comparison_QW_animated_WTtitle.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-10T21:38:24+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/images/AI_acc_comparison_QW_animated_WTtitle.gif" />
<meta property="twitter:title" content="Neural Network Accelerators (2024)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Quentin Wach"},"dateModified":"2024-04-10T21:38:24+02:00","datePublished":"2024-04-10T21:38:24+02:00","description":"An short overview of mostly commercial AI hardware. A rough comparison of ASICs, FPGAs, GPUs, and looking at compute requirements and energy consumption. I end this post with the realization that we need to take a first-principles physics approach to narrow down what future AI accelerator designs will have to look like because the market is complex and largely driven by momentum.","headline":"Neural Network Accelerators (2024)","image":"http://localhost:4000/images/AI_acc_comparison_QW_animated_WTtitle.gif","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/personal/2024/04/10/AI-chip-market.html"},"url":"http://localhost:4000/personal/2024/04/10/AI-chip-market.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">

  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.png" ><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Q. Wach" /><script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>
</head>

</head><body onload="document.body.style.opacity='1'"><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Q. Wach</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about">About</a><a class="page-link" href="/personal">Blog</a><a class="page-link" href="/side-projects">Side-Projects</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div style="height: 50px;">
          </br>
        </div>
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural Network Accelerators (2024)</h1><p class="post-meta">by 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Quentin Wach</span></span>
      
      
        • 
        22
       min read for 
        4031
       words •
      <time class="dt-published" datetime="2024-04-10T21:38:24+02:00" itemprop="datePublished">
        Apr 10, 2024
      </time></p>
  </header>

  <div>
    <div class="post-content e-content" itemprop="articleBody">
      <style>
    img[alt=AIAccComp] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px; }
    img[alt=AIAccComp]:hover {
                            transform: scale(1);
                            box-shadow: 0px 0px 0px rgba(0, 0, 0, 0);
                            z-index: 10000;
    }
</style>

<p><img src="/images/AI_acc_comparison_QW_animated_WT.gif" alt="AIAccComp" />
<span style="font-size: 14px;">
    Speed of computing as dependent on the consumed power for different hardware architectures: GPUs, digital ASICs, mixed signal ASICs, and FPGAs. I adapted and animated this figure based on a figure I found and saved quite some time ago <sup id="fnref:MFigure" role="doc-noteref"><a href="#fn:MFigure" class="footnote" rel="footnote">1</a></sup>.
</span></p>

<h2 id="introduction">Introduction</h2>
<p>Artificial general intelligence (AGI)
<button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
    Google’s Deep Mind characterizes current general AIs as <strong>emerging AGI</strong>. Level 1. Examples, their researchers say, include: ChatGPT, Bard, Llama 2, and Gemini <sup id="fnref:GoogleAGI" role="doc-noteref"><a href="#fn:GoogleAGI" class="footnote" rel="footnote">2</a></sup>.
</span>
is speculated to arrive within only a couple of years from now. And with its emergence comes a plethora of issues. In these notes, I want to focus not on the consequences of AGI, nor what algorithms and software will lead to it, but rather the bottlenecks and problems there are especially regarding its hardware.</p>

<blockquote>
  <p>“We may recall that the first steam engine had less than 0.1% efficiency and it took 200 years (1780 to 1980) to develop steam engines that reached biological efficiency.”</p>
</blockquote>

<p>writes Ruch et al. in 2011<sup id="fnref:RuchBioAI" role="doc-noteref"><a href="#fn:RuchBioAI" class="footnote" rel="footnote">3</a></sup>.
Initial utility is typically achieved through effectiveness. We build something that gets the job done. And we do not mind how long it takes or how much it costs as long as it fixes the problem we couldn’t otherwise solve. We saw and see the same thing happening with the rise of the digital computer and now with modern AI based on artificial neural networks. CPUs and GPUs are general tools. Architectures meant to accomplish whatever task we throw at it. And time is <em>the</em> enemy of any technology company. Accelerate or be out-competed. Companies are willing to spend millions to innovate and train giant neural networks and billions to build the required infrastructure just to stay in the game. Efficiency is or has been an afterthought. But it is essential for making technologies widely available and allow for future technologies to build upon it!</p>

<p>In 2020-2023, the bottleneck for AI was arguably the amount of hardware available due to a global chip shortage so vast it made history<sup id="fnref:ChipShortage" role="doc-noteref"><a href="#fn:ChipShortage" class="footnote" rel="footnote">4</a></sup><sup id="fnref:ChipShortage2" role="doc-noteref"><a href="#fn:ChipShortage2" class="footnote" rel="footnote">5</a></sup>. 
In 2024, managing the voltage conversion is still a major issue for scaling up<sup id="fnref:MuskVTransformers" role="doc-noteref"><a href="#fn:MuskVTransformers" class="footnote" rel="footnote">6</a></sup>.
<button class="sidenote-button-left"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-left">
    <strong>Voltage transformers</strong> play a crucial role in transmitting electrical power over long distances. To achieve this, voltage is stepped up for transmission and stepped down for safe use in homes, offices, and data centers. These transformers are bulky and expensive but necessary for safety and equipment protection. Demand for both types of transformers has surged since 2020 due to the needs of AI startups, data centers, and renewable energy sources like solar and wind farms<sup id="fnref:TransformerBottleneck" role="doc-noteref"><a href="#fn:TransformerBottleneck" class="footnote" rel="footnote">7</a></sup><sup id="fnref:TFormer1" role="doc-noteref"><a href="#fn:TFormer1" class="footnote" rel="footnote">8</a></sup><sup id="fnref:TFormer2" role="doc-noteref"><a href="#fn:TFormer2" class="footnote" rel="footnote">9</a></sup>. A topic worth its own article.
</span>
With these things hopefully more or more under control the future bottleneck will simply be the availability and cost of energy. In the short term, this means: how can we get the required energy cheapely? But as we continue to scale it also means: How can we make our hardware more energy efficient? How do we get to the top left quadrant of this figure? How do we achieve high performance for low power? Software optimization does not cut it. The hardware is too general to be efficient. And among other things the von-Neumann bottleneck<sup id="fnref:vonNeumannBottle" role="doc-noteref"><a href="#fn:vonNeumannBottle" class="footnote" rel="footnote">10</a></sup> fundamentally limits what these computers can do.
<button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
There is of course also a <strong>data problem</strong>. Our algorithms are still not good enough to generalize from as little data as humans do. And books don’t contain as much as one might expect though it is of a high quality. The internet is far larger though. So then we may look at videos and podcasts etc. But will it be enough? Grog3 will require 100.000 H100s to train coherently. And at that point, there really is a lack of avaiable data, including text, video, and synthetic data. Much more is needed to satisfy the training needs of such large models. Read<sup id="fnref:TokenOverview" role="doc-noteref"><a href="#fn:TokenOverview" class="footnote" rel="footnote">11</a></sup> to learn more about how much training data is out there.
</span></p>

<p>AI software has become seriously awesome and…</p>

<blockquote>
  <p>“People who are really serious about software should make their own hardware.” Alan Kay<sup id="fnref:AKayHard" role="doc-noteref"><a href="#fn:AKayHard" class="footnote" rel="footnote">12</a></sup>.</p>
</blockquote>

<h2 id="compute">Compute</h2>
<blockquote>
  <p>“The base metric is becoming compute. Not money.” Harrison Kinsley (aka Sentdex)<sup id="fnref:ComputeNotMoney1" role="doc-noteref"><a href="#fn:ComputeNotMoney1" class="footnote" rel="footnote">13</a></sup>.</p>
</blockquote>

<p>This statement has been repeated over and over by engineers and technologists in and outside of Silicon Valley, including Sam Altman saying</p>

<blockquote>
  <p>“I think compute is going to be the currency of the future. I think it’ll be maybe the most precious commodity in the world.”<sup id="fnref:SamAltmanLex1" role="doc-noteref"><a href="#fn:SamAltmanLex1" class="footnote" rel="footnote">14</a></sup></p>
</blockquote>

<p><em>“Compute”</em> or computing power is simply a general term to describe how much computations can be done in practice to solve our problems or, simplified even further, it is a measure of how powerful a computer is<sup id="fnref:WikiCompute" role="doc-noteref"><a href="#fn:WikiCompute" class="footnote" rel="footnote">15</a></sup>. Today’s GPUs reach several TFLOPs (Tera-FLOP) with a FLOP being a <em>“floating point operation”</em>. It has to be considered though that these can be 64-, 32-, 16-, 8-, or even just 4-bit operations! Nonetheless, 1 TFLOP = \(1\cdot10^{12}\) FLOP. The question is now how many operations can be done per second.
<button class="sidenote-button-left"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-left">
    The <strong>scaling hypothesis in AI</strong> posits that as the amount of data and computational resources available for training a machine learning model increases, the performance of the model also improves, often in a predictable manner. This hypothesis suggests that many AI tasks, particularly those related to deep learning, benefit from scaling up both the size of the dataset used for training and the computational power available for training the model.
    In practice, this means that larger neural networks trained on more extensive datasets tend to achieve better performance, such as higher accuracy or improved generalization, compared to smaller models trained on smaller datasets. The scaling hypothesis has been supported by empirical evidence in various domains, particularly in natural language processing, computer vision, and other areas where deep learning techniques are prevalent<sup id="fnref:ScalingHypo" role="doc-noteref"><a href="#fn:ScalingHypo" class="footnote" rel="footnote">16</a></sup><sup id="fnref:ScalingHypo3" role="doc-noteref"><a href="#fn:ScalingHypo3" class="footnote" rel="footnote">17</a></sup><sup id="fnref:ScalingHypo2" role="doc-noteref"><a href="#fn:ScalingHypo2" class="footnote" rel="footnote">18</a></sup>.
    <!--
    <style>
        img[alt=centerAI] { float: left; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    ![centerAI](/images/scaling_hypothesis.jpg) -->
</span></p>
<style>
    img[alt=centerAI] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px; }
    img:hover {
                            transform: scale(1.9);
                            box-shadow: 0px 1px 3px rgba(0, 0, 0, 0.25);
                            z-index: 10000;
}
</style>

<p><img src="/images/AI_compute.png" alt="centerAI" />
<span style="font-size: 14px;">
    The amount of compute required to train (a) neural network and do inference (b) after training.
    This figure was created by myself but the data was compiled by Jaime Sevilla et al.<sup id="fnref:AIDemand_Data_1" role="doc-noteref"><a href="#fn:AIDemand_Data_1" class="footnote" rel="footnote">19</a></sup><sup id="fnref:AIDemand_Data_2" role="doc-noteref"><a href="#fn:AIDemand_Data_2" class="footnote" rel="footnote">20</a></sup>.
</span></p>

<blockquote>
  <p>“Extrapolating the spectacular performance of GPT-3 into the future suggests that the answer to life, the universe and everything is just 4.398 trillion parameters.” Geoffrey Hinton<sup id="fnref:HintonScaling" role="doc-noteref"><a href="#fn:HintonScaling" class="footnote" rel="footnote">21</a></sup>.</p>
</blockquote>

<h3 id="training">Training</h3>
<p>Subplot (a) of the figure presented above shows the insane increase in training compute over time to create more and more capable AIs, rising orders of magnitude within years.</p>

<blockquote>
  <p>“[…] since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with <strong>a 3.4-month doubling time</strong> (by comparison, Moore’s Law had a 2-year doubling period). Since 2012, this metric has grown by more than <strong>300,000x</strong> (a 2-year doubling period would yield only a 7x increase).” This was previously observed by OpenAI<sup id="fnref:OpenAIComputePost" role="doc-noteref"><a href="#fn:OpenAIComputePost" class="footnote" rel="footnote">22</a></sup>.</p>
</blockquote>

<p>As we can see in the figure above, this is not only true for the largest AIs but for AI in general!</p>

<h3 id="inference">Inference</h3>
<p><button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
    <style>
        img[alt=NAchip] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    In fact, the <strong>North American AI chipset market</strong> is growing much faster for edge compute (which is essentially infernce) compared to cloud compute (mostly training) <sup id="fnref:NAChipMarket1" role="doc-noteref"><a href="#fn:NAChipMarket1" class="footnote" rel="footnote">23</a></sup>.
    <img src="/images/NA_AI_chipmarket.png" alt="NAchip" />
    In a live stream, George Hotz mockingly said:
    <em>“All those fucks are trying to make edge AI. […] Look! The AI inference market is bigger than the AI training market! So we should all go for the inference market! It’s way easier! […] Easier and bigger! […] There are a hundred little fucks who make inference chips that no one really wants and only one who is making training chips (NVIDIA).”</em> <sup id="fnref:GHotz_InferenceMarket" role="doc-noteref"><a href="#fn:GHotz_InferenceMarket" class="footnote" rel="footnote">24</a></sup>
    <style>
        img[alt=H100Buyers] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    <img src="/images/H100_buyers.png" alt="H100Buyers" />
    <em>Estimated 2023 H100 shipments by end customer as reported by Omida Research <sup id="fnref:OmidaH100Shipments" role="doc-noteref"><a href="#fn:OmidaH100Shipments" class="footnote" rel="footnote">25</a></sup>.</em>
</span>
Contrary to training, the compute for inference has hardly grown in comparison. That is because we want the inference to be fast and cheap in order for it to be practical. There can’t be a seconds of delay for the self-driving car. We don’t want our LLMs to spend hours computing an hour for our question and burn the energy of a small town for it. So with traditional hardware we are quite limited here and models are optimized to work within these boundaries. Imagine what we could do with better dedicated hardware though! Imagine LLMs would respond instantly and run locally on your phone. This will be necessary for robots especially. So while inference does not seem like a growth market at first glance, the demand is absolutely there.
<button class="sidenote-button-left"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-left">
    Update: OpenAI released the o1 model recently which uses chain-of-thought self-prompting to deliver higher quality answers at the cost of a longer waiting time and significantly higher token consumption<sup id="fnref:OpenAI_o1" role="doc-noteref"><a href="#fn:OpenAI_o1" class="footnote" rel="footnote">26</a></sup>. A new scaling law was discovered similar to the training scaling laws that shows that the accuracy rises as we spend more time “thinking”. The financial price for using these models is huge. We can therefore expect a much greater demand for better inference. Where I had doubt before if inference hardware made sense, I don’t have this doubt anymore! <em>September 18, 2024</em>
</span></p>

<h2 id="applications">Applications</h2>
<p>A figure from the paper showing how much it currently costs even to do just inference.
A. S. Luccioni et al. recently compared several machine learning tasks in terms of their energy consumption and nicely showed that image generation is orders of magnitudes more costly during inference compared to other ML tasks like image classification or text generation<sup id="fnref:AI_EnergyInference" role="doc-noteref"><a href="#fn:AI_EnergyInference" class="footnote" rel="footnote">27</a></sup>:</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>Inference Energy (kWh)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Text Classification</td>
      <td>0.002 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Extractive QA</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Masked Language Modeling</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Toke Classification</td>
      <td>0.004 \(\pm\) 0.002</td>
    </tr>
    <tr>
      <td>Image Classification</td>
      <td>0.007 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td>0.04 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>0.05 \(\pm\) 0.03</td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>0.49 \(\pm\) 0.01</td>
    </tr>
    <tr>
      <td>Image Captioning</td>
      <td>0.63 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Image Generation</td>
      <td>2.9 \(\pm\) 3.3</td>
    </tr>
  </tbody>
</table>

<h2 id="compute--energy--human-vs-gpt4">Compute / Energy:  Human vs. GPT4</h2>
<p>At this point, I want to highlight the title figure at the beginning of these notes showing the speed in GOP/s over power in W of GPUs, FPGAs, digital and mixed signal ASICs. More than any other figure here, I think it gives a sense of how the different hardware compares.</p>

<p><button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
I hope to soon go more into depth discussing limits like Landauer’s principle<sup id="fnref:LandauP" role="doc-noteref"><a href="#fn:LandauP" class="footnote" rel="footnote">28</a></sup> as it relates to this.
</span>
The sad truth as of now is that there seems to be a hard limit for the amount of compute per second we can achieve for the energy used. To drive down the costs or AI developement and inference, we’d have to dramatically reduce the cost of energy, which would require an energy revolution in of itself, or rethink and rebuild our hardware on a much more fundamental level than is currently done (in the industry). That is precisely why I am writing these notes.</p>

<p>Comparisons to the human brain and how it developed are often made, especially now in context of LLMs and their extreme popularity and energy consumption because it requires historic energy consumption to train useful LLMs. Even if we compare all the energy consumed by a human throughout their entire life, the difference is vast. Some napkin math:</p>

<p>Assuming that the calorie intake is 2000 kcal / day \(\times\) 365 days where 1163 Wh = 1000 kcal<sup id="fnref:kcal_in_Watt" role="doc-noteref"><a href="#fn:kcal_in_Watt" class="footnote" rel="footnote">29</a></sup><sup id="fnref:kcal_in_Watt2" role="doc-noteref"><a href="#fn:kcal_in_Watt2" class="footnote" rel="footnote">30</a></sup>, that’s 730.000 kcal / year or \(8.5 \cdot 10^{-5}\) GWh/year per person. Now multiply this times the decades humans take to learn what LLMs know and we can maybe subtract or or two orders of magnitude in energy consumption. Still, in comparison, OpenAI used 50 GWh to train GPT4<sup id="fnref:GPT4_energy" role="doc-noteref"><a href="#fn:GPT4_energy" class="footnote" rel="footnote">31</a></sup>utilizing 25.000 NVIDIA A100 GPUs that run around 6.5 kW each.
It doesn’t take a math genius to see that there are orders of magnitude in difference. And as a result, the training cost for GPT4 was around $100.000.000 over a period of around 100 days (roughly $65M for the pre-training alone) as stated Sam Altman himself<sup id="fnref:GPT4_price" role="doc-noteref"><a href="#fn:GPT4_price" class="footnote" rel="footnote">32</a></sup><sup id="fnref:GPT4_facts" role="doc-noteref"><a href="#fn:GPT4_facts" class="footnote" rel="footnote">33</a></sup>.</p>

<h2 id="general-use-of-hardware">General Use of Hardware</h2>
<h3 id="gpus-nvidia-leads-the-way">GPUs: NVIDIA Leads the Way</h3>
<p>NVIDIA is the non-plus ultra for AI training at the largest scale. We see this reflected with commercial GPUs providing the fastest training. The drawback: These GPUs also consume by far the most power.</p>

<p>In NVIDIA’s most recent developer conference, Jensen Huang bragged about how hot these GPUs get. So hot, that they need to be liquid cooled (which poses new challenges in data centres). So hot, that the coolant coming out after just seconds could be used as a whirlpool, I remember him stating. Later on stage, he then continued talking about how NVIDIAs new accelerators are now much more energy efficient<sup id="fnref:NVIDIAGTC2024" role="doc-noteref"><a href="#fn:NVIDIAGTC2024" class="footnote" rel="footnote">34</a></sup>. It takes a special kind of genius to pull of such contradiction marketing. I am not even trying to be critical. It’s rather humorous!</p>

<p>Even with their incredible energy demands and the high training costs that follow, NVIDIAs glowing hot GPUs have a bright future.</p>

<p>That is because GPUs offer versatility and power for a wide range of applications. GPUs excel in scenarios where the workload is diverse or evolving, as they are capable of handling various computational tasks beyond AI, including graphics rendering, scientific simulations, and data analytics, all of which can compliment each other. They are <em>the</em> research plattform. Additionally ,they are simply extremely widespread. Nearly every computer has a GPU. Many have NVIDIA GPUs. And NVIDIA made sure to build the entire stack from the bottom up to the higher software layers to make it as easy for developers and scientists to get as much performance without needing to write much code.</p>

<h3 id="field-programmable-gate-arrays-fpgas-for-prototyping">Field-Programmable Gate Arrays (FPGAs) for Prototyping</h3>
<p>If we were to compete, lowering energy consumption is the angle of attack. FPGAs are, in part, competitive enough in terms of speed yet at similar power consumption and much, much worse usability. Nonetheless, some argue for a future of FPGAs:</p>

<blockquote>
  <p>“Due to their software-defined nature, FPGAs offer easier design flows and shorter time to market accelerators than an application-specific integrated circuit (ASIC).”<sup id="fnref:FPGACentres" role="doc-noteref"><a href="#fn:FPGACentres" class="footnote" rel="footnote">35</a></sup></p>
</blockquote>

<p>The advantage lies for tasks that require less compute but desire better optimization in speed and energy efficiency hence inference. And FPGAs are typically easier to bring to market than ASICs. So in scenarios where power consumption must be tightly controlled, such as in embedded systems or edge computing devices, FPGAs may provide a compelling solution. But compelling enough?</p>

<h3 id="application-specific-integrated-circuits-asics-for-on-the-edge-devices">Application-Specific Integrated Circuits (ASICs) for on the Edge Devices</h3>
<p>ASICs after all are by far superior for addressing energy concerns in AI hardware. Unlike general-purpose GPUs, ASICs are tailored specifically for certain tasks, maximizing efficiency and performance for those tasks while minimizing power consumption. On the other hand, ASICs entail higher initial development costs and longer time-to-market. Since progress in AI is driven primarily by new developments in software, developing ASICs that are general enough to keep up with that progress but specific enough to actually have an advantage over GPUs is tricky.</p>

<h3 id="tensor-processing-units-tpus-for-deep-learning-training">Tensor Processing Units (TPUs) for Deep Learning Training</h3>
<p>Developed by Google, TPUs are designed specifically for accelerating machine learning workloads, particularly those that involve neural network inference and training by focusing on matrix multiplication operations fundamental to neural network computations. And by doing so, TPUs are able to achieve remarkable speedups while consuming significantly less power compared to traditional GPU-based solutions. As I see it, they are simply not as widely available and their lack of generality compared to GPUs may make the development of AI applications a bit more difficult compared to GPUs once again.</p>

<h3 id="neural-processing-units-npus-for-inference">Neural Processing Units (NPUs) for Inference</h3>
<p>Similarly, NPUs represent a specialized class of AI hardware optimized specifically for neural network computations. They are designed to accelerate the execution of machine learning workloads but, and this is the crucial difference, <strong>with a focus on inference tasks</strong> commonly found in applications such as computer vision, natural language processing, and speech recognition. By incorporating dedicated hardware accelerators for key operations involved in neural network inference, such as convolutions and matrix multiplications, NPUs are able to achieve significant speedups while consuming less power. So the idea is similar to TPUs but with a focus more on inference specifically for edge devices and private computers.</p>

<h2 id="conclusion">Conclusion</h2>
<!--
+ Photonics is great at communication but has a lot of fundamental issues that keep it from being practical for computing in comparison electronics at least for now and likely the next 5-10 years. Integrating photonics as intra- and interconnects will be crucial for high-performance computing though. But it is not yet a bottleneck.

+ Thermodynamic computing, to me, seems mostly academic for now.

+ Quantum computing seems extremely promising but simply scaling this technology will take more decades as well.

+ Analog electronic computing on the other hand just works. The issue here is not the fundamental technology but rather our ability to design analog ICs quickly! It is my quess that analog electronics should also naturally play well with photonics. Very large scale integration (VLSI) is a resulting challenge. 
  >"Conventional analog designs require schematic-entry based specification and **a custom layout design that requires many iterations to meet design goals**."[^ADesign]

+ While I believe GPUs will continue to dominate the training market for the next 3+ years while digital ASICs, and to some degree FPGAs, will increasingly dominate the market for AI inference. But if we take a larger view of humanity and the required compute in the next decades and centuries, analog computing will have to become a dominating force. Not for general computing but for AI acceleration. And not just inference but, importantly, training as well.
-->

<p>What I and others ask is <strong>what is the computer of the future?</strong> I started writing this down because there are so many approaches to AI accelerators. Most quite traditional, simply designing digital ASICs. Some more exotic. Yet they are all extremely confident in that <em>their</em> approach is what the future AI super-computer will look like. That is why one of my I will now be more focused on the physics of computing. I’ll try and look at it from a first-principles standpoint, look at where our corrent technological limitations and lie to then give my personal estimate on what is the most sensible solution we should be working on.
<button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
    <strong>Thank you</strong> for reading these notes of mine as I tried to clear up my thinking on the subject. In the end, it lead me to the believe that it is rather pointless to try and directly compare all the available hardware. There are too many nuances and the technical details matter a lot. Yet, it is unclear how and what exactly matters and will be the most important metric to focus on for future computers. It seems to be quite clear to me that current computer designs are primarily driven by the econonmy and not a long-term focus and reasoning from first principles. So I hope to do a better job and illuminate this subject a bit more in a future post that takes a more physics oriented approach. Another lesson I took away for my own work and design process is that I need to iterate faster so I can prove my designs wrong faster as well. <br /> - Quentin
</span></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:MFigure" role="doc-endnote">
      <p>Adapted based on a figure originally linked to <a href="https://NICSEFC.EE.TSINGHUA.EDU.CN">https://NICSEFC.EE.TSINGHUA.EDU.CN</a>. Yet, I was not able to find it again as the link seemed to have been broken. <a href="#fnref:MFigure" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GoogleAGI" role="doc-endnote">
      <p>https://arxiv.org/pdf/2311.02462.pdf <a href="#fnref:GoogleAGI" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:RuchBioAI" role="doc-endnote">
      <p><a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://www.zurich.ibm.com/pdf/news/Towards_5D_Scaling.pdf">P. Ruch et al., <em>Toward five-dimensionalscaling: How densityimproves efficiency infuture computers</em>, IBM, 2011</a> <a href="#fnref:RuchBioAI" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ChipShortage" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/2020%E2%80%932023_global_chip_shortage <a href="#fnref:ChipShortage" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ChipShortage2" role="doc-endnote">
      <p>https://edition.cnn.com/2023/08/06/tech/ai-chips-supply-chain/index.html <a href="#fnref:ChipShortage2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:MuskVTransformers" role="doc-endnote">
      <p>Interview with Elon Musk on X.com: https://twitter.com/i/spaces/1YqJDgRydwaGV/peek Musk mentioned the various bottlenecks of AI in other recent interviews as well. <a href="#fnref:MuskVTransformers" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TransformerBottleneck" role="doc-endnote">
      <p>https://finance.yahoo.com/news/electrical-transformers-could-giant-bottleneck-220423599.html <a href="#fnref:TransformerBottleneck" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TFormer1" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/Data_center <a href="#fnref:TFormer1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TFormer2" role="doc-endnote">
      <p>https://www.olsun.com/power-integrator-data-center-2/ <a href="#fnref:TFormer2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:vonNeumannBottle" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/Von_Neumann_architecture <a href="#fnref:vonNeumannBottle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TokenOverview" role="doc-endnote">
      <p>https://www.educatingsilicon.com/2024/05/09/how-much-llm-training-data-is-there-in-the-limit/ <a href="#fnref:TokenOverview" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AKayHard" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Alan_Kay">Alan Kay</a>, <em><a href="https://www.folklore.org/Creative_Think.html">Creative Think</a></em>, 1982 <a href="#fnref:AKayHard" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ComputeNotMoney1" role="doc-endnote">
      <p><a href="https://x.com/Sentdex/status/1773358212403654860?s=20">Harrison Kinsley on X.com: https://x.com/Sentdex/status/1773358212403654860?s=20</a> <a href="#fnref:ComputeNotMoney1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:SamAltmanLex1" role="doc-endnote">
      <p>Sam Altman on the Lex Fridman podcast: <a href="https://lexfridman.com/sam-altman-2-transcript/">https://lexfridman.com/sam-altman-2-transcript/</a> <a href="#fnref:SamAltmanLex1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:WikiCompute" role="doc-endnote">
      <p><a href="Wikipedia: Floating Point Operations Per Second, https://de.wikipedia.org/wiki/Floating_Point_Operations_Per_Second">Wikipedia: Floating Point Operations Per Second, https://de.wikipedia.org/wiki/Floating_Point_Operations_Per_Second</a> <a href="#fnref:WikiCompute" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ScalingHypo" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/1712.00409.pdf?source=content_type%3Areact%7Cfirst_level_url%3Aarticle%7Csection%3Amain_content%7Cbutton%3Abody_link">J. Hestness et al., <em>DEEPLEARNING SCALING IS PREDICTABLE, EMPIRICALLY</em>, https://arxiv.org/pdf/1712.00409.pdf?source=content_type%3Areact%7Cfirst_level_url%3Aarticle%7Csection%3Amain_content%7Cbutton%3Abody_link, 2017</a> <a href="#fnref:ScalingHypo" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ScalingHypo3" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2001.08361.pdf">J. Kaplan et al., <em>Scaling Laws for Neural Language Models</em>, ArXiv, 2020: https://arxiv.org/pdf/2001.08361.pdf</a> <a href="#fnref:ScalingHypo3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ScalingHypo2" role="doc-endnote">
      <p>Gwern actually wrote about the scaling hypothesis on his blog as well: <a href="https://gwern.net/scaling-hypothesis">https://gwern.net/scaling-hypothesis</a> <a href="#fnref:ScalingHypo2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_1" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2202.05924.pdf">Sevilla et al., <em>Compute Trends Across Three Eras of Machine Learning</em>, ArXiv, 2022</a> <a href="#fnref:AIDemand_Data_1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_2" role="doc-endnote">
      <p><a href="https://epochai.org/blog/compute-trends">Epoch AI, <em>Compute Trends</em>, https://epochai.org/blog/compute-trends</a> <a href="#fnref:AIDemand_Data_2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:HintonScaling" role="doc-endnote">
      <p><a href="https://twitter.com/geoffreyhinton/status/1270814602931187715">Geoffrey Hinton on Twitter in 2020: https://twitter.com/geoffreyhinton/status/1270814602931187715</a> <a href="#fnref:HintonScaling" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:OpenAIComputePost" role="doc-endnote">
      <p>OpenAI, AI and compute: https://openai.com/research/ai-and-compute <a href="#fnref:OpenAIComputePost" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:NAChipMarket1" role="doc-endnote">
      <p>https://www.eetasia.com/ai-chip-market-to-reach-70b-by-2026/ <a href="#fnref:NAChipMarket1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GHotz_InferenceMarket" role="doc-endnote">
      <p>George Hotz, https://www.youtube.com/watch?v=iXupOjSZu1Y <a href="#fnref:GHotz_InferenceMarket" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:OmidaH100Shipments" role="doc-endnote">
      <p>Omida Research. (I was unable to find the original source though similar data can be found here: https://www.tomshardware.com/tech-industry/nvidia-ai-and-hpc-gpu-sales-reportedly-approached-half-a-million-units-in-q3-thanks-to-meta-facebook) <a href="#fnref:OmidaH100Shipments" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:OpenAI_o1" role="doc-endnote">
      <p>https://openai.com/o1/ <a href="#fnref:OpenAI_o1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AI_EnergyInference" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2311.16863.pdf">Luccioni et al., <em>Power Hungry Processing: Watts Driving the Cost of AI Deployment?</em>, ArXiv, 2023</a> <a href="#fnref:AI_EnergyInference" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:LandauP" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/Landauer%27s_principle <a href="#fnref:LandauP" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:kcal_in_Watt" role="doc-endnote">
      <p>Feel free to convert back and forth between kcal and Wh yourself at <a href="https://convertlive.com/u/convert/kilocalories-per-hour/to/watts#2000">https://convertlive.com/u/convert/kilocalories-per-hour/to/watts#2000</a>. <a href="#fnref:kcal_in_Watt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:kcal_in_Watt2" role="doc-endnote">
      <p>https://de.wikipedia.org/wiki/Kalorie <a href="#fnref:kcal_in_Watt2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GPT4_energy" role="doc-endnote">
      <p>https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air <a href="#fnref:GPT4_energy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GPT4_price" role="doc-endnote">
      <p>Knight, Will. “OpenAI’s CEO Says the Age of Giant AI Models Is Already Over”. Wired. Archived from the original on April 18, 2023. Retrieved April 18, 2023 – via www.wired.com. https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/ <a href="#fnref:GPT4_price" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GPT4_facts" role="doc-endnote">
      <p>https://patmcguinness.substack.com/p/gpt-4-details-revealed <a href="#fnref:GPT4_facts" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:NVIDIAGTC2024" role="doc-endnote">
      <p><a href="https://www.youtube.com/watch?v=Y2F8yisiS6E&amp;list=PLZHnYvH1qtOYPPHRaHf9yPQkIcGpIUpdL">NVIDIA GTC March 2024 Keynote with Jensen Huang: https://www.youtube.com/watch?v=Y2F8yisiS6E&amp;list=PLZHnYvH1qtOYPPHRaHf9yPQkIcGpIUpdL</a> <a href="#fnref:NVIDIAGTC2024" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:FPGACentres" role="doc-endnote">
      <p>https://www.allaboutcircuits.com/news/shifting-to-a-field-programable-gate-array-data-center-future/ <a href="#fnref:FPGACentres" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>
  </div>


<!--
<h4>Did You Enjoy Reading This Article?</h4>
<p>Here are some more articles/posts you might like as well:</p>





<ul>
    

        
        

        

        
    

        
        

        

        
            <li><a href="/science-engineering/2024/08/28/image-ranker.html">Image Ranker: A Web App for Pairwise Image Ranking</a></li>
            

            
        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
            <li><a href="/personal/2024/02/15/dubins-paths-for-waveguide-routing.html">Dubins Paths for Waveguide Routing</a></li>
            

            
        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
    

        
        

        

        
            <li><a href="/2018/02/12/GAN.html">Generative Adversarial Network</a></li>
            

            
        
    
</ul>
-->


  <div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT 
     *  THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR 
     *  PLATFORM OR CMS.
     *  
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: 
     *  https://disqus.com/admin/universalcode/#configuration-variables
     */
    var disqus_config = function () {
        // Replace PAGE_URL with your page's canonical URL variable
        this.page.url = 'http://localhost:4000/personal/2024/04/10/AI-chip-market.html';  
        
        // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        this.page.identifier = '/personal/2024/04/10/AI-chip-market'; 
    };
    
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');
        
        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://quentinwach.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript><a class="u-url" href="/personal/2024/04/10/AI-chip-market.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">

      <div class="footer-col">
        <p></p>
      </div>
    
    </div>

    <div class="social-links">
    
      <i>"I just wondered how things were put together."</i> Claude Shannon.
      </br>

      Thank you for visiting!
      You can reach out to me on social media<ul class="social-media-list"><li><a href="https://www.twitter.com/quentinwach"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">quentinwach</span></a></li></ul>
.
    </div>

  </div>

</footer><script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>
        $(document).ready(function(){
            $('.tag-filter a').click(function(e){
                e.preventDefault();
                var tag = $(this).attr('href').substring(1);
                $('.post').hide();
                $('.post[data-tags~="' + tag + '"]').show();
            });
        });
    </script>


  </body>

</html>