<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Analog Electronic Artificial Neural Network for Extreme Energy-Efficient Inference | Q. Wach</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Analog Electronic Artificial Neural Network for Extreme Energy-Efficient Inference" />
<meta name="author" content="Quentin Wach" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A dense artificial neural network is trained and then converted to an analog electronic circuit simulated with SPICE using Python. While a fully analog implementation of a neural network shows great promise in terms of energy efficiency and speed, a hybrid approach is necessary to make it programmable. This is an introduction. As such, I also discuss the advantages and problems of analog computing. (BM.1, the first version, is promising. BM.2 will be an 130 nm IC design. Work in progress.)" />
<meta property="og:description" content="A dense artificial neural network is trained and then converted to an analog electronic circuit simulated with SPICE using Python. While a fully analog implementation of a neural network shows great promise in terms of energy efficiency and speed, a hybrid approach is necessary to make it programmable. This is an introduction. As such, I also discuss the advantages and problems of analog computing. (BM.1, the first version, is promising. BM.2 will be an 130 nm IC design. Work in progress.)" />
<link rel="canonical" href="http://localhost:4000/science-engineering/2024/02/26/training-MNIST.html" />
<meta property="og:url" content="http://localhost:4000/science-engineering/2024/02/26/training-MNIST.html" />
<meta property="og:site_name" content="Q. Wach" />
<meta property="og:image" content="http://localhost:4000/images/physics-processing-unit/machine_brain_2.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-26T20:38:24+01:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/images/physics-processing-unit/machine_brain_2.png" />
<meta property="twitter:title" content="Analog Electronic Artificial Neural Network for Extreme Energy-Efficient Inference" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Quentin Wach"},"dateModified":"2024-02-26T20:38:24+01:00","datePublished":"2024-02-26T20:38:24+01:00","description":"A dense artificial neural network is trained and then converted to an analog electronic circuit simulated with SPICE using Python. While a fully analog implementation of a neural network shows great promise in terms of energy efficiency and speed, a hybrid approach is necessary to make it programmable. This is an introduction. As such, I also discuss the advantages and problems of analog computing. (BM.1, the first version, is promising. BM.2 will be an 130 nm IC design. Work in progress.)","headline":"Analog Electronic Artificial Neural Network for Extreme Energy-Efficient Inference","image":"http://localhost:4000/images/physics-processing-unit/machine_brain_2.png","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/science-engineering/2024/02/26/training-MNIST.html"},"url":"http://localhost:4000/science-engineering/2024/02/26/training-MNIST.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Rounded:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400..700;1,400..700&display=swap" rel="stylesheet">

  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.png" ><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Q. Wach" /><script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>
</head>

</head><body onload="document.body.style.opacity='1'"><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Q. Wach</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about">About</a><a class="page-link" href="/science_and_engineering">Science &amp; Engineering</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div style="height: 50px;">
          </br>
        </div>
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Analog Electronic Artificial Neural Network for Extreme Energy-Efficient Inference</h1><p class="post-meta">by 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Quentin Wach</span></span>
      
      
        • 
        21
       min read for 
        3835
       words •
      <time class="dt-published" datetime="2024-02-26T20:38:24+01:00" itemprop="datePublished">
        Feb 26, 2024
      </time></p>
  </header>

  <div>
    <div class="post-content e-content" itemprop="articleBody">
      <ul>
  <li>Convert dense neural network in PyTorch to analog electric circuit based on op-amps.</li>
  <li>Implement non-linear activation functions as op-amp based neural network with linear activation functions.</li>
  <li>Discussion of the Landauer limit: In order to increase compute / energy beyond the limit, we must either make computation reversible or reduce the number of bits used. One way to reduce the number of bits is to simply go partially or even fully analog. Or we can stick to digital and make the logic reversible, meaning, no bits are wasted. Correct?</li>
</ul>

<!-- IMAGE: "/images/physics-processing-unit/quentinwach_A_modern_machine_brain_made_of_glass_metal_transist_c0e95e7c-154f-422d-b562-a1d5a2845fa7.png" -->
<!-- Challenges of Analog AI Accelerators -->
<!-- Read the <a href="/pdfs/Nanoscale_2024_2_13_SA.pdf"><button class="PDFButton">PDF</button></a>. -->
<p>Try the <a href="https://github.com/QuentinWach/QuentinWach/blob/master/README.md"><button class="PDFButton">Code on GitHub</button></a>.
<span class="sidenote-left">
This rather laborious post is meant to be somewhat of a starting point and reference for me to point to when friends with other interests and expertise want to know what I have been up to. At the same time, I am just starting to figure a lot of things out as well. I think this whole topic is not only awesome but important to at least think about.
</span>
<em>A deep artificial neural network (DNN) is trained and then reimplemented as an analog electronic circuit simulated with Python. The Python library is developed to converts the DNN design into an analog hardware design and simulate its operation. That is because an actual physical implementation is extremely costly due to the high cost of the individual components, notably the operational amplifiers, digital to analog converters and digital potentiometers. While a fully analog implementation of a neural network shows great promise in terms of energy efficiency and potentially speed, scaling this technology is not only challenging due to accumulating inaccuracies but also due to the high component costs. (10 TFLOPS at 4 W.)</em></p>
<div class="tag_list"> 
    <div class="tag">machine learning</div>
    <div class="tag">artificial intelligence</div>
    <div class="tag">python</div>
    <div class="tag">analog computing</div>
    <div class="tag">electronics</div>
    <div class="tag">neural networks</div>
    <div class="tag">hardware design</div>
    <div class="tag">computer engineering</div>
</div>
<style>
    img[alt=Board] {float: left; width: 100%; border-radius:5px; margin-right: 10px; margin-bottom: 30px; margin-top: 5px;}
</style>

<p><img src="/images/mem_analog_2024.png" alt="Board" /></p>

<p><span class="sidenote-left">
    <style>
        img[alt=me] {float: left; width: 100%; border-radius:5px; margin-right: 10px; margin-bottom: 30px; margin-top: 5px;}
    </style>
    <img src="/images/memes/floating_me.webp" alt="me" />
    Many thanks for reading!
</span></p>

<!--
Work through the following articles:
+ https://openaccess.thecvf.com/content/WACV2023W/WVAQ/papers/Ornhag_Accelerating_AI_Using_Next-Generation_Hardware_Possibilities_and_Challenges_With_Analog_WACVW_2023_paper.pdf
+ https://dl.acm.org/doi/pdf/10.1145/3453688.3461746?casa_token=4grhyhx2cCsAAAAA:Ho4pkCjh2oT-lwe99qegHoQLMOscAuKZ6KNCkipYP1CapWV4gvF2y8mEVAuZ7wG3zNNJA6J_v6ZFSA
+ https://www.sciencedirect.com/science/article/pii/S2095809921003349
+ https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864008&casa_token=ZKjaCrd4dj8AAAAA:4aMxRshEMCS2yJC1LBm9QFHq50gyIRrWsNaqg89gCeY8RD1qUCu1vxMCrvW3-FrThMoYq8jRB0c&tag=1
+ https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180545&casa_token=0XZ9QgOkFt4AAAAA:N7a31G75sAtUAwO2a6z3ID-R7GIz-DFKT15B1pv6XgM6aRNUR2caaT5TB_wVYJmIyHZKMeuK5f0
-->

<blockquote>
  <p>“People who are really serious about software should make their own hardware.” Alan Kay <sup id="fnref:AKayHard" role="doc-noteref"><a href="#fn:AKayHard" class="footnote" rel="footnote">1</a></sup></p>
</blockquote>

<p>The purpose of this project is to implement a neural network capable of classifying MNIST nunbers at competitive accuracies
as an analog electronic circuit. As such we will:</p>

<ol>
  <li>Build and train a dense neural network in Keras.</li>
  <li>Convert that model into a SPICE netlist with analog modules.</li>
  <li>Simulate the analog electronic ANN with PySPICE using NGSpice as backend.</li>
  <li>Discuss the results of the simulations compared to digital and other analog approaches for AI acceleration.</li>
  <li>Discuss issues, limitations, and desirable features.</li>
  <li>The general context of analog VLSI design, the next step of layout engineering using tools like magic, the Sky130 PDK and multi-wafer projects (MWPs) to produce an actual IC.</li>
</ol>

<h2 id="1-introduction">1 Introduction</h2>

<h3 id="compute-performance">Compute Performance</h3>
<blockquote>
  <p>“You’re trying to scale compute flops.
I’m trying to get rid of flops entirely.
We are not the same.” Guillaume Verdon (aka Beff Jezos) <sup id="fnref:TweetBeffJezos" role="doc-noteref"><a href="#fn:TweetBeffJezos" class="footnote" rel="footnote">2</a></sup></p>
</blockquote>

<p>Let us say that every mathematical operation (OP) is equivalent to a floating point operation (FLOP). Then we can simply look at the schematic of analog neural network and count the number of operators or rather elements that are operated on in one feedforward pass. We then measure or calculate how much time the network would require to do this calculation. This gives us:
<span class="sidenote-right">
    At least that is my intution but it might very well be that OPs and FLOPs really can’t be compared this directly.
</span></p>

\[\begin{equation}
    500.000~\text{FLOP} / 1~\mu\text{s}\\ = 500.000 \times 10^{6}~\text{FLOPS}\\= 5 \times 10^{11}~\text{FLOPS}\\ = 0.5 \times \text{TFLOPS} 
\end{equation}\]

<h3 id="energy-consumption">Energy Consumption</h3>
<p>At the same time, we can calculate how much \(\text{W}\) is consumed for that pass.</p>

<h3 id="compute-in-the-age-of-ai">Compute in the Age of AI</h3>

<h4 id="what-is-compute">What is Compute?</h4>
<blockquote>
  <p>“The base metric is becoming compute. Not money.” Harrison Kinsley (aka Sentdex) <sup id="fnref:ComputeNotMoney1" role="doc-noteref"><a href="#fn:ComputeNotMoney1" class="footnote" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>“Compute”</em> or computing is simply a general term to describe how much computational power is available. So it is a measure of how powerful a computer is <sup id="fnref:WikiCompute" role="doc-noteref"><a href="#fn:WikiCompute" class="footnote" rel="footnote">4</a></sup>. Today’s GPUs reach several TFLOPs (Tera-FLOP) with a FLOP being a <em>“floating point operation”</em>. It has to be considered though that these can be 64-, 32-, 16-, 8-, or even just 4-bit operations! Nonetheless, 1 TFLOP = \(1\cdot10^{12}\) FLOP. The question is now how many operations can be done per second as well what KIND of operations. Multiplication? Addition? Some functional transformation?</p>
<style>
    img[alt=centerAI] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px;}
</style>

<p><img src="/images/AI_compute.png" alt="centerAI" />
<span style="font-size: 14px;">
    The amount of compute required to train (a) neural network and do inference (b) after training.
    This figure was created by myself but the data was compiled by Jaime Sevilla et al. <sup id="fnref:AIDemand_Data_1" role="doc-noteref"><a href="#fn:AIDemand_Data_1" class="footnote" rel="footnote">5</a></sup> <sup id="fnref:AIDemand_Data_2" role="doc-noteref"><a href="#fn:AIDemand_Data_2" class="footnote" rel="footnote">6</a></sup>.
</span></p>

<h4 id="the-arms-race-of-training-ais">The Arms Race of Training AIs</h4>

<h4 id="do-not-underestimate-the-market-and-importance-of-inference">Do Not Underestimate the Market and Importance of Inference</h4>
<p>Contrary to training, the compute for inference has hardly grown in comparison. That is because we want the inference to be fast and cheap in order for it to be practical. There can’t be a seconds of delay for the self-driving car. We don’t want our LLMs to spend hours computing an hour for our question and burn the energy of a small town for it. So with traditional hardware we are quite limited here and models are optimized to work within these boundaries. Imagine what we could do with better dedicated hardware though! Imagine LLMs would respond instantly and run locally on your phone. This will be necessary for robots especially. So while inference does not seem like a growth market at first glance, the demand is absolutely there.
<span class="sidenote-right">
    <style>
        img[alt=NAchip] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    In fact, the North American AI chipset market is growing much fast for edge compute (which is essentially infernce) compared to cloud compute (mostly training) <sup id="fnref:NAChipMarket1" role="doc-noteref"><a href="#fn:NAChipMarket1" class="footnote" rel="footnote">7</a></sup>.
    <img src="/images/NA_AI_chipmarket.png" alt="NAchip" />
    In a live stream, George Hotz mockingly said:
    <em>“All those fucks are trying to make edge AI. […] Look! The AI inference market is bigger than the AI training market! So we should all go for the inference market! It’s way easier! […] Easier and bigger! […] There are a hundred little fucks who make inference chips that no one really wants and only one who is making training chips (NVIDIA).”</em> <sup id="fnref:GHotz_InferenceMarket" role="doc-noteref"><a href="#fn:GHotz_InferenceMarket" class="footnote" rel="footnote">8</a></sup>
    <style>
        img[alt=H100Buyers] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    <img src="/images/H100_buyers.png" alt="H100Buyers" />
    <em>Estimated 2023 H100 shipments by end customer as reported by Omida Research <sup id="fnref:OmidaH100Shipments" role="doc-noteref"><a href="#fn:OmidaH100Shipments" class="footnote" rel="footnote">9</a></sup>.</em>
</span></p>

<h3 id="energy-consumption-1">Energy Consumption</h3>

<p>My own figure based on this data.</p>

<p>A figure from the paper showing how much it currently costs even to do just inference.
A. S. Luccioni et al. recently compared several machine learning tasks in terms of their energy consumption and nicely showed that image generation is orders of magnitudes more costly during inference compared to other ML tasks like image classification or text generation <sup id="fnref:AI_EnergyInference" role="doc-noteref"><a href="#fn:AI_EnergyInference" class="footnote" rel="footnote">10</a></sup>:</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>Inference Energy (kWh)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Text Classification</td>
      <td>0.002 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Extractive QA</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Masked Language Modeling</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Toke Classification</td>
      <td>0.004 \(\pm\) 0.002</td>
    </tr>
    <tr>
      <td>Image Classification</td>
      <td>0.007 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td>0.04 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>0.05 \(\pm\) 0.03</td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>0.49 \(\pm\) 0.01</td>
    </tr>
    <tr>
      <td>Image Captioning</td>
      <td>0.63 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Image Generation</td>
      <td>2.9 \(\pm\) 3.3</td>
    </tr>
  </tbody>
</table>

<p>10^23 FLOP for 10^6 USD thus 10^17 FLOP / USD = 10^5 TFLOP / USD or rather</p>

\[\$1 = 10^{17}~\text{FLOP} \\
\$1 = 100 \cdot \text{PFLOP}\]

<p>Figure comparing different hardware including the biological brain.</p>

<p><img src="/images/AI_acc_comparison_QW_animated.gif" alt="AIAccComp" /></p>

<p>AI Impacts writes</p>
<blockquote>
  <p>“In November 2017, we estimate the price for one GFLOPS to be between $0.03 and $3 for single or double precision performance, using GPUs (therefore excluding some applications). Amortized over three years, this is $1.1 x 10-5 -$1.1 x 10-7 /GFLOPShour” <sup id="fnref:AiImpacts" role="doc-noteref"><a href="#fn:AiImpacts" class="footnote" rel="footnote">11</a></sup>.</p>
</blockquote>

<h3 id="physics-imposed-limitations">Physics-Imposed Limitations</h3>
<p>What should be physically possible?</p>

<h3 id="accelerators">Accelerators</h3>

<p>As of now, GPUs are the way to go to train and run deep neural networks as they are by far the fastest, most common, available, and easy to use processors. All praise NVIDIA. Yet there are also ASICs like Google’s TPUs created specifically to improve the performance just for neural networks. Indeed, they require less power while being similarly fast. Still, the fastest training is, weirdly, achieved with GPUs. Then there are FPGAs that similar to ASICs achieve a lower power consumption at similar performance. Both ASICs and FPGAs are less general though. - More importantly, all available processors either consume a lot of power or have low performance. Ideally, we would like to get 100 OPS for less than 2 Watt, similarly to the brain.</p>

<p>I believe part of the solution are photonics as a way to speed up data transmission and bandwidth. But computation is extremely difficult and it will take much more time until photonic computers enter the competition in this particular field. We can use analog electronics though as well which, while less accurate, achieve incredible power-efficiency at high speeds! The key challenge here is thus dealing with noise and errors especially as we scale up the analog electronic circuit size.</p>

<p>I can’t seem to find much information on this topic online nor in books. So I will hack this together myself.</p>

<h3 id="digital-vs-analog">Digital vs Analog</h3>
<p>Most accelerators as of now still use digital signals to ensure accuracy and direct compatibility with existing technologies as the conversion from digital to analog and analog to digital is a science and art in itself. This leaves a wide opening to try and play with analog approaches though.</p>

<!-- <a href="https://everycircuit.com/circuit/5270808553062400">Analog Implementation of a Novel Resistive-Type Sigmoidal Neuron</a><br> -->
<iframe width="100%" height="500px" style="border-radius: 5px;" src="https://everycircuit.com/embed/5270808553062400" frameborder="0"></iframe>

<h2 id="analog-computing">Analog Computing</h2>
<h3 id="why-analog-circuits">Why Analog Circuits?</h3>
<p>Nowadays, most people don’t study analog computing.</p>

<p>Yet, we live in an analog world! And so, in order to do digital computing, we need to constantly convert from analog to digital and back, a very challenging task that some people make a living from just doing this alone! The reason digital computers are everywhere is because of their precision. They are reliable. They are therefore also easily scalable because new components do not introduce more and more errors. Hence why we now have over a hundred-billion transistors within our modern processors only a few cm in diameter. This is much more challenging to do with analog circuits. Note that quantum computing and photonic computing are predominately analog computing as well!</p>

<p>The great disadvantage with digital computing is that it requires a clock which can only operate at a very limited speed that hasn’t increased in recent decades and more importantly, digital circuits are extremely computation heavy and so consume a lot of energy.</p>

<p>Yet, for example, in analog computing using a memristors, matrix multiplication is almost trivial to do and done in a single shot at an extremely low energy consumption!</p>

<p><span class="sidenote-left">
    <strong>How to compare analog to digital compute?</strong> Analog computation has the advantage of not being limited to ones and zeroes as it computes directly with the floating values. This means we can also directly compare our compute with digital computers. This also highlights one of the main advantages of analog computing: 
    Lack of complexity! We do not need to <em>hundreds</em> of transistors to express a single number. It is directly encoded in our signals.
    <style>
        img[alt=sidenote] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px;}
    </style>
    <img src="/images/omni_man_analog_meme.png" alt="sidenote" />
</span></p>

<h3 id="operational-amplifier-as-a-black-box">Operational Amplifier as a Black Box</h3>
<p>The typical operational amplifier has two inputs \(V_{\text{in\pm}}\) and one output \(V_{\text{out}}\). Then</p>

\[V_{\text{out}} = A_0 \cdot (V_{\text{in+}} - V_{\text{in+}}).\]

<p>So what the op-amp cares about is the difference between the applied input voltages.
For the ideal case, we assume that the impedances are \(Z_{\text{in}} = \infty\) and \(Z_{\text{out}} = 0\) for the ideal op-amp. Besides this we want to run the operations at high speed and so we desire a bandwidth \(BW = \infty\) and a slew rate (the response rate) equaulling infinity. The output swing is supposed to be infinity as well. And finally, the gain. Sometimes it is desired to be infinity but it is actually depending on the definition. What do we mean by “ideal amplifier”? Here, we will wish it to be infinity as well, though. None of these goals/ideals can be achieved of course. But these are the goals.</p>

<!-- 
[WebLink about OpAmps](https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Instrumental_Analysis_(LibreTexts)/03%3A_Operational_Amplifiers_in_Chemical_Instrumentation_(TBD)/3.04%3A_Application_of_Operational_Amplifiers_to_Mathematical_Operations)

The increase in computation! (Also compare with the phoenix project paper!)
https://www.visualcapitalist.com/cp/charted-history-exponential-growth-in-ai-computation/
Better data:
https://arxiv.org/pdf/2202.05924.pdf
Costs of training: 
https://spectrum.ieee.org/state-of-ai-2023

https://ocw.mit.edu/courses/6-071j-introduction-to-electronics-signals-and-measurement-spring-2006/5bc88e4af14bc9c72e63f48cd2a44613_23_op_amps2.pdf

https://arxiv.org/pdf/2107.06283.pdf

https://www.analog.com/media/en/training-seminars/tutorials/MT-079.pdf

https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4

https://www.youtube.com/watch?v=Qgjawf20v7Y

https://www.youtube.com/watch?v=kbVqTMy8HMg

https://www.youtube.com/watch?v=RTtpUi-gQOg

https://www.youtube.com/watch?v=VQoyypYTz2U

Neural Networks on FPGAs:
https://www.youtube.com/@marcowinzker3682

Still to watch:
https://www.youtube.com/watch?v=brhOo-_7NS4

https://ars.copernicus.org/articles/19/105/2021/

https://www.youtube.com/watch?v=VWn6Ixh2eDg

https://www.youtube.com/watch?v=VWn6Ixh2eDg

https://www.youtube.com/watch?v=GVsUOuSjvcg&t=898s

MYTHIC Analog computing interview:
https://www.youtube.com/watch?v=7PJJM__zbbE
Mythic is now shipping its analog AI inference processor. 25 TOPS at 3 watts!?  Crazy!
80.000 ADCs (analog to digitial converters)
High accuracy
Mass production hitting soon (if not already).
Last hurdle: Software usability.

Brain inspired Computing Nature Paper: FUNDAMENTAL!
https://www.nature.com/articles/s41467-019-12521-x

Von-Neumann Computing: Super important Nature paper, too!
https://www.nature.com/collections/dhdjceebhg

Still watch:
https://www.youtube.com/watch?v=EwueqdgIvq4

First full analog chip for machine learning:
https://www.aspinity.com/aml100
Yet it is mostly aimed at IOT applications!
How is it programmable? Field programmable? Is it an analog FPGA or smth?!

MUST READ FOR ELECTRONICS!
https://github.com/kitspace/awesome-electronics

Why are OPAMPS so expensive?
https://open.spotify.com/intl-de/track/164VgxTozx99XCinCB9ITR?si=d850164a39d54909


Not only are OPAMPS expensive. Digital potentiometers are even more expensive! But how do we load in our data then?

We can use the MCP4011-202E/MS digital potentiometer found on
https://www.arrow.com/en/products/mcp4011-202ems/microchip-technology?region=nac&utm_currency=USD. Yet, with around 800 input signals, this would cost us roughly 400€ given the price of 0,50€ per potentiometer.

Rather than building our own DAC (digital-to-analog converter) with so many outputs, we should attempt to find an already highly integrated pre-build DAC on the market. Typically, their output channel number is not greater than 32 or 64 though which means we'll need to multiplex.

4 channels

100 = 170€
340€ for 800 channels... This sucks...
https://www.digikey.de/en/products/detail/microchip-technology/MCP4728T-E-UN/5358293


Why are there so many fucking different OPAMS?
See the blog on my HHI laptop.

Why people don't think about analog computing. Yet it is everywhere!
https://electronics.stackexchange.com/questions/595199/what-is-the-actual-niche-for-operational-amplifiers-these-days


This paper shows how difficult it is to design, simulate, and actually build such an analog neural network as they only created a simulation as well! And they did so for a CMOS!
https://drive.google.com/file/d/1aGEucNV2uK2J1UHzV5xi8G5DK6MYNnQh/view
-->

<h3 id="analog-electronic-artificial-neuron">Analog Electronic Artificial Neuron</h3>
<p>It is actually extremely fascinating to think about spiking neural networks but given that these are as of now very uncommon
in state-of-the-art (SOTA) AIs, let’s focus on how we can build a simple perceptron-like artificial neural with analog electronics.</p>

<p>I wanted to keep this first experiment under a budget of 50€ because I am broke as hell. Most opamps sadly cost around $30ct though! But not all. We can use a LM258DRG4 by Texas Instruments which only costs $0.072 / piece (https://www.ti.com/product/LM258/part-details/LM258DRG4?HQS=ocb-tistore-invf-invftransact-invf-store-octopart-wwe found on https://octopart.com/search?category_id=4252&amp;start=790). If we have 300 opamps that’s less than 25€.</p>

<h2 id="dense-neural-network">Dense Neural Network</h2>
<p>We can recognize handwritten numbers using deep neural networks<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">12</a></sup> quite easily using convolutional layers<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">13</a></sup> but I will focus only on densily connected neurons for now. The dataset commonly used for this task is the famous <em>MNIST</em><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">14</a></sup> dataset. All of our code will be written in Python<sup id="fnref:Python" role="doc-noteref"><a href="#fn:Python" class="footnote" rel="footnote">15</a></sup>. Let’s begin with importing some libraries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">PyTorch</span> <span class="k">as</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">matplitlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>We then define the architecture of our network:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># input layer
</span>
<span class="c1"># hidden layers
</span>
<span class="c1"># output layer
</span>
</code></pre></div></div>

<h3 id="error-discussion">Error Discussion</h3>

<p>The first Perceptron was, in fact, implemented using a custom analog computer at the time[^PerceptronAnalog] (See Veritasium.).</p>

<blockquote>
  <p>“The perceptron was invented in 1943 by Warren McCulloch and Walter Pitts[^]. The first hardware implementation was Mark I Perceptron machine built in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt,[^] […]” (Wikipedia[^].)</p>
</blockquote>

<!-- 
<style>
    img[alt=Omniman] { float: right; width: 50%; border-radius:15px; padding-left: 10px;, padding-bottom: 10px; padding-top: 10px;}
</style>
![Omniman](/images/omni_man_analog_meme.png)
-->

<h3 id="analog-circuits">Analog Circuits</h3>

<p><strong>Signal Generators.</strong>
As it turns out, signal generators, implementing functions with analog circuitry, is a non-trivial task in its own right which has its own research field dedicated to it. But since our goal here is a <em>“simple”</em> ASIC with only a few non-linear functions, this should be manageable, right? <em>RIGHT?!</em></p>

<p><strong>Leaky ReLu.</strong></p>

<p><strong>Sigmoid.</strong></p>

<h3 id="creating-the-printed-circuit-board-pcb">Creating the Printed Circuit Board (PCB)</h3>
<p>In order to learn how to create custom PCBs, I have been watching and working through <a href="https://www.youtube.com/watch?v=aVUqaB0IMh4">this course by Philip Salmony (Phil’s Lab)</a>. The whole project is based on the STM32 microcontroller.</p>

<p>If you are already proficient in PCB design, skip this section because I will be going through some fairly basic stuff, I am sure. That’s because, at this point, wasn’t trained to build one and so I will also share here how I learned to design PCBs.</p>

<ul>
  <li>What is a micro-controller?</li>
  <li>What can the STM32 do / why did we choose it?</li>
  <li>Why do we add each of these capacitors?</li>
  <li>Why do we have a different source for analog voltage compared to the digital voltage?</li>
  <li>Why do we add a ferrite bead? To dissipate heat? Why is that necessary?</li>
  <li>Why is the internal crystal oscillator so bad that we have to add our own one? What does that mean?</li>
</ul>

<p>Integrated Development Environment for STM32:</p>

<h3 id="simulating-with-ltspice-and-python">Simulating with LTSpice and Python</h3>
<p>A quick search on YouTube doesn’t offer much when it comes to learning how to build analog electronic circuits. But I found this devlog: https://www.youtube.com/watch?v=lKwzFdG2–8&amp;list=PL_R4uxT5thflWVbSWtl-rx5_C_q0RxjyV&amp;index=5 which brought me to learn about</p>
<ul>
  <li>https://www.analog.com/en/resources/media-center/videos/series/ltspice-essentials-tutorial.html</li>
  <li>https://pypi.org/project/PyLTSpice/
software I will built on, to help me design the rather complex analog neural network and simulate it. If this does not work, I will just code it all from scratch or dive into IBMs analog neural network design &amp; simulation Python library [source?!].</li>
</ul>

<h3 id="visualizing-the-design">Visualizing the design</h3>
<p>To make this super awesome, let’s create a visualization of the design like here https://www.youtube.com/watch?v=0Fixr39X8S4 as well as the input and output of the network! 😁
For this, once again, I wrote a little library.</p>

<h3 id="whats-next">What’s Next?</h3>
<p>The great challenge of analog designs overall is managing noise. This project did not bother with that problem at all since it is a rather tiny prototype. But in order to scale it from a few hundred neurons to millions, we will need to come up with a way to either eliminate the noise problem or make it a feature just like it is in the human brain. Both are interesting directions though the latter would make more sense if we actually move away from artificial neural networks to spiking neural networks.</p>

<p>Due to the cost of the components, miniaturization will also be essential. But so far, so good!</p>

<div style="{background-color: 'crimson'; border-radius: 15px;}">
### Get One!
Thank you for reading 😁. (Or even just scrolling through all of this.) If you found this interesting, I encourage you to go and check out
    <div>
    <button href="" class="PDFButton">all the files on Github (free)</button>
    or
    <button href="" class="PDFButton">get a working and tested board ($49,99 + shipping)</button>
    </div>
</div>

<!--
### THE GREAT FUTURE CHALLENGE #1: Making it programmable. Thus allowing for upload of weights in a cheap way! (This problem was solved already by at least one company.)

### THE GREAT FUTURE CHALLENGE #2: Making it precise. Adding ADCs and DACs as well as a calibration loop to ensure that all the signals are as close to the desired, simulated signal as possible to minimize the added error.

### THE GREAT FUTURE CHALLENGE #3: Make a module that is scalable to the TFLOP regime at 3+GHz!

### THE GREAT FUTURE CHALLENGE #4: Allow for fast backpropagation and traininig.

### GFC #5: Create an API so that any Numpy or PyTorch-based ANN can be trained with this chip.

### GFC #6: Create a PCB that can be plugged into and controlled by a computer with a CPU and OS. How can we connect multiple chips and distribute training across them?
-->

<p>For a general AI training accelerator, I could make it support tinygrad first since it has a tiny set of operations:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Buffer                                                       # class of memory on this device
unary_op  (NOOP, EXP2, LOG2, CAST, SIN, SQRT)                # A -&gt; A
reduce_op (SUM, MAX)                                         # A -&gt; B (smaller size, B has 1 in shape)
binary_op (ADD, SUB, MUL, DIV, CMPEQ, MAX)                   # A + A -&gt; A (all the same size)
load_op   (EMPTY, CONST, FROM, CONTIGUOUS, CUSTOM)           # -&gt; A   (initialize data on device)
ternary_op (WHERE)                                           # A, A, A -&gt; A
</code></pre></div></div>
<p>and machine learning operations:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Relu, Log, Exp, Sin                            # unary ops
Sum, Max                                       # reduce ops (with axis argument)
Maximum, Add, Sub, Mul, Pow, Div, Equal        # binary ops (no broadcasting, use expand)
Expand, Reshape, Permute, Pad, Shrink, Flip    # movement ops
Where                                          # ternary ops
</code></pre></div></div>
<p>See the documentation <sup id="fnref:tinygrad" role="doc-noteref"><a href="#fn:tinygrad" class="footnote" rel="footnote">16</a></sup>.
I have no idea how to implment these in a hybrid computer right now. One step at a time!
(Also, how would we go about supporting PyTorch, Tensorflow, Keras, Numpy, …? Integrate some RISC-V?)</p>

<h3 id="references">References</h3>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:AKayHard" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Alan_Kay">Alan Kay</a>, <em><a href="https://www.folklore.org/Creative_Think.html">Creative Think</a></em>, 1982 <a href="#fnref:AKayHard" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TweetBeffJezos" role="doc-endnote">
      <p>Guillaume Verdon, https://twitter.com/BasedBeffJezos/status/1775445241434431541 <a href="#fnref:TweetBeffJezos" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ComputeNotMoney1" role="doc-endnote">
      <p><a href="https://x.com/Sentdex/status/1773358212403654860?s=20">Harrison Kinsley on X.com: https://x.com/Sentdex/status/1773358212403654860?s=20</a> <a href="#fnref:ComputeNotMoney1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:WikiCompute" role="doc-endnote">
      <p>https://de.wikipedia.org/wiki/Floating_Point_Operations_Per_Second <a href="#fnref:WikiCompute" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_1" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2202.05924.pdf"><em>COMPUTE TRENDS ACROSS THREE ERAS OF MACHINE LEARNING</em>, ArXiv</a> <a href="#fnref:AIDemand_Data_1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_2" role="doc-endnote">
      <p>https://epochai.org/blog/compute-trends <a href="#fnref:AIDemand_Data_2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:NAChipMarket1" role="doc-endnote">
      <p>https://www.eetasia.com/ai-chip-market-to-reach-70b-by-2026/ <a href="#fnref:NAChipMarket1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GHotz_InferenceMarket" role="doc-endnote">
      <p>George Hotz, https://www.youtube.com/watch?v=iXupOjSZu1Y <a href="#fnref:GHotz_InferenceMarket" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:OmidaH100Shipments" role="doc-endnote">
      <p>Omida Research. (I was unable to find the original source though similar data can be found here: https://www.tomshardware.com/tech-industry/nvidia-ai-and-hpc-gpu-sales-reportedly-approached-half-a-million-units-in-q3-thanks-to-meta-facebook) <a href="#fnref:OmidaH100Shipments" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AI_EnergyInference" role="doc-endnote">
      <p>https://arxiv.org/pdf/2311.16863.pdf <a href="#fnref:AI_EnergyInference" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AiImpacts" role="doc-endnote">
      <p>https://aiimpacts.org/current-flops-prices/ <a href="#fnref:AiImpacts" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>A <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>B <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>C <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:Python" role="doc-endnote">
      <p>Python <a href="#fnref:Python" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tinygrad" role="doc-endnote">
      <p>https://github.com/QuentinWach/tinygrad/blob/master/docs/adding_new_accelerators.md <a href="#fnref:tinygrad" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    </div>
  </div>



  <!--
  <div id="disqus_thread"></div>
<script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT 
     *  THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR 
     *  PLATFORM OR CMS.
     *  
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: 
     *  https://disqus.com/admin/universalcode/#configuration-variables
     */
    /*
    var disqus_config = function () {
        // Replace PAGE_URL with your page's canonical URL variable
        this.page.url = PAGE_URL;  
        
        // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        this.page.identifier = PAGE_IDENTIFIER_2; 
    };
    */
    
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');
        
        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://EXAMPLE.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>
  --><!---->


  <a class="u-url" href="/science-engineering/2024/02/26/training-MNIST.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">

      <div class="footer-col">
        <p></p>
      </div>
    
    </div>

    <div class="social-links">
      Wow, you scrolled all the way to the bottom!

      Maybe you are interested in my more <a href="/personal" class="button">personal blog posts</a>, too, then!
      
      Or reach out to me on social media:<ul class="social-media-list"><li><a href="https://github.com/quentinwach"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">quentinwach</span></a></li><li><a href="https://www.linkedin.com/in/quentinwach"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">quentinwach</span></a></li><li><a href="https://www.twitter.com/QuentinWach"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">QuentinWach</span></a></li></ul>
</div>

  </div>

</footer><script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>
        $(document).ready(function(){
            $('.tag-filter a').click(function(e){
                e.preventDefault();
                var tag = $(this).attr('href').substring(1);
                $('.post').hide();
                $('.post[data-tags~="' + tag + '"]').show();
            });
        });
    </script>


  </body>

</html>