<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-09T20:11:05+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Q. Wach</title><subtitle></subtitle><entry><title type="html">Field-Dependent THz Transport Nonlinearities in Semiconductor Nano Structures</title><link href="http://localhost:4000/science-engineering/2024/04/22/second-publication.html" rel="alternate" type="text/html" title="Field-Dependent THz Transport Nonlinearities in Semiconductor Nano Structures" /><published>2024-04-22T21:38:24+02:00</published><updated>2024-04-22T21:38:24+02:00</updated><id>http://localhost:4000/science-engineering/2024/04/22/second-publication</id><content type="html" xml:base="http://localhost:4000/science-engineering/2024/04/22/second-publication.html"><![CDATA[]]></content><author><name>Quentin Wach</name></author><category term="science-engineering" /><category term="physics" /><category term="optics" /><category term="THz spectroscopy" /><category term="thesis" /><category term="density matrix" /><category term="quantum mechanics" /><category term="semiconductors" /><category term="nanoscience" /><summary type="html"><![CDATA[The charge transport in semiconductor quantum dots and nanorods is studied theoretically, predicting strong field-dependent nonlinear mobility effects and intra-pulse gain. We studied the temperature- and size-sensitive mobility spectra, crucial for applications like 6G tech and nano electronics in general.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/NS_Title_Graphic_Square_3.png" /><media:content medium="image" url="http://localhost:4000/images/NS_Title_Graphic_Square_3.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Augmenting Obsidian.md with AI: Automatic Linking, Tagging, and Grouping</title><link href="http://localhost:4000/science-engineering/2024/04/16/AI-obsidian.html" rel="alternate" type="text/html" title="Augmenting Obsidian.md with AI: Automatic Linking, Tagging, and Grouping" /><published>2024-04-16T00:00:00+02:00</published><updated>2024-04-16T00:00:00+02:00</updated><id>http://localhost:4000/science-engineering/2024/04/16/AI-obsidian</id><content type="html" xml:base="http://localhost:4000/science-engineering/2024/04/16/AI-obsidian.html"><![CDATA[]]></content><author><name>[&quot;Quentin Wach&quot;]</name></author><category term="science-engineering" /><category term="python" /><category term="obsidian" /><category term="productivity" /><category term="AI" /><summary type="html"><![CDATA[A simple, free and open Python script to quickly organize your markdown notes in Obsidian.md using OpenAI's large language models. Any random, untagged, empty, or unlinked notes? No more. This little script will fill the gaps, connect what belongs together, and even create high level organizing nodes if a specific topic has a lot of notes.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/obsidian-AI/1x1_image.png" /><media:content medium="image" url="http://localhost:4000/images/obsidian-AI/1x1_image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Space of Neural Network Accelerators</title><link href="http://localhost:4000/science-engineering/2024/04/10/AI-chip-market.html" rel="alternate" type="text/html" title="The Space of Neural Network Accelerators" /><published>2024-04-10T21:38:24+02:00</published><updated>2024-04-10T21:38:24+02:00</updated><id>http://localhost:4000/science-engineering/2024/04/10/AI-chip-market</id><content type="html" xml:base="http://localhost:4000/science-engineering/2024/04/10/AI-chip-market.html"><![CDATA[<style>
    img[alt=AIAccComp] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px; }
    img[alt=AIAccComp]:hover {
                            transform: scale(1);
                            box-shadow: 0px 0px 0px rgba(0, 0, 0, 0);
                            z-index: 10000;
    }
</style>

<p><img src="/images/AI_acc_comparison_QW_animated_WT.gif" alt="AIAccComp" />
<span style="font-size: 14px;">
    Speed of computing as dependent on the consumed power for different hardware architectures: GPUs, digital ASICs, mixed signal ASICs, and FPGAs. I adapted and animated this figure based on a figure I found and saved quite some time ago <sup id="fnref:MFigure" role="doc-noteref"><a href="#fn:MFigure" class="footnote" rel="footnote">1</a></sup>.
</span></p>

<h2 id="1-introduction">1. Introduction</h2>
<p>Artificial general intelligence (AGI)
<button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
    Google’s Deep Mind characterizes current general AIs as <strong>emerging AGI</strong>. Level 1. Examples, their researchers say, include: ChatGPT, Bard, Llama 2, and Gemini <sup id="fnref:GoogleAGI" role="doc-noteref"><a href="#fn:GoogleAGI" class="footnote" rel="footnote">2</a></sup>.
</span>
is speculated to arrive within only a couple of years from now. And with its emergence comes a plethora of issues. In these notes, I want to focus not on the consequences of AGI, nor what algorithms and software will lead to it, but rather the bottlenecks and problems there are especially regarding its hardware.</p>

<blockquote>
  <p>“We may recall that the first steam engine had less than 0.1% efficiency and it took 200 years (1780 to 1980) to develop steam engines that reached biological efficiency.”</p>
</blockquote>

<p>writes Ruch et al. in 2011<sup id="fnref:RuchBioAI" role="doc-noteref"><a href="#fn:RuchBioAI" class="footnote" rel="footnote">3</a></sup>.
<button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">Where <em>“efficiency”</em> means how much energy is consumed proportional to the power of the engine.</span>
Initial utility is typically achieved through effectiveness. We build something that gets the job done. And we do not mind how long it takes or how much it costs as long as it fixes the problem we couldn’t otherwise solve. We saw and see the same thing happening with the rise of the digital computer and now with modern AI based on artificial neural networks. CPUs and GPUs are general tools. Architectures meant to accomplish whatever task we throw at it. And time is <em>the</em> enemy of any technology company. Accelerate or be out-competed. Companies are willing to spend millions to innovate and train giant neural networks and billions to build the required infrastructure just to stay in the game. Efficiency is or has been an afterthought. But it is essential for making technologies widely available and allow for future technologies to build upon it!</p>

<p>In 2020-2023, the bottleneck for AI was arguably the amount of hardware available due to a global chip shortage so vast it made history<sup id="fnref:ChipShortage" role="doc-noteref"><a href="#fn:ChipShortage" class="footnote" rel="footnote">4</a></sup><sup id="fnref:ChipShortage2" role="doc-noteref"><a href="#fn:ChipShortage2" class="footnote" rel="footnote">5</a></sup>. 
In 2024, managing the voltage conversion is still a major issue for scaling up<sup id="fnref:MuskVTransformers" role="doc-noteref"><a href="#fn:MuskVTransformers" class="footnote" rel="footnote">6</a></sup>.
<button class="sidenote-button-left"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-left">
    <strong>Voltage transformers</strong> play a crucial role in transmitting electrical power over long distances. To achieve this, voltage is stepped up for transmission and stepped down for safe use in homes, offices, and data centers. These transformers are bulky and expensive but necessary for safety and equipment protection. Demand for both types of transformers has surged since 2020 due to the needs of AI startups, data centers, and renewable energy sources like solar and wind farms<sup id="fnref:TransformerBottleneck" role="doc-noteref"><a href="#fn:TransformerBottleneck" class="footnote" rel="footnote">7</a></sup><sup id="fnref:TFormer1" role="doc-noteref"><a href="#fn:TFormer1" class="footnote" rel="footnote">8</a></sup><sup id="fnref:TFormer2" role="doc-noteref"><a href="#fn:TFormer2" class="footnote" rel="footnote">9</a></sup>. A topic worth its own article.
</span>
With these things hopefully more or more under control the future bottleneck will simply be the availability and cost of energy. In the short term, this means: how can we get the required energy cheapely? But as we continue to scale it also means: How can we make our hardware more energy efficient? How do we get to the top left quadrant of this figure? How do we achieve high performance for low power? Software optimization does not cut it. The hardware is too general to be efficient. And among other things the von-Neumann bottleneck<sup id="fnref:vonNeumannBottle" role="doc-noteref"><a href="#fn:vonNeumannBottle" class="footnote" rel="footnote">10</a></sup> fundamentally limits what these computers can do.
<button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
There is of course also a <strong>data problem</strong>. I think I heard Musk say recently that we can fit all of the books in the world on just a couple of harddrives if represented as tokens. So that amount of data doesn’t seem enough. Our algorithms are still not good enough to generalize from that little data. The internet is far larger though. So then we may look at videos and podcasts etc. But will it be enough? Grog3 will require 100.000 H100s to train coherently. And at that point, there really is a lack of avaiable data, including text, video, and synthetic data. Much more is needed to satisfy the training needs of such large models. 
</span></p>

<p>AI software has become seriously awesome and…</p>

<blockquote>
  <p>“People who are really serious about software should make their own hardware.” Alan Kay<sup id="fnref:AKayHard" role="doc-noteref"><a href="#fn:AKayHard" class="footnote" rel="footnote">11</a></sup>.</p>
</blockquote>

<h2 id="2-compute">2. Compute</h2>
<blockquote>
  <p>“The base metric is becoming compute. Not money.” Harrison Kinsley (aka Sentdex)<sup id="fnref:ComputeNotMoney1" role="doc-noteref"><a href="#fn:ComputeNotMoney1" class="footnote" rel="footnote">12</a></sup>.</p>
</blockquote>

<p>This statement has been repeated over and over by engineers and technologists in and outside of Silicon Valley, including Sam Altman saying</p>

<blockquote>
  <p>“I think compute is going to be the currency of the future. I think it’ll be maybe the most precious commodity in the world.”<sup id="fnref:SamAltmanLex1" role="doc-noteref"><a href="#fn:SamAltmanLex1" class="footnote" rel="footnote">13</a></sup></p>
</blockquote>

<p><em>“Compute”</em> or computing power is simply a general term to describe how much computations can be done in practice to solve our problems or, simplified even further, it is a measure of how powerful a computer is<sup id="fnref:WikiCompute" role="doc-noteref"><a href="#fn:WikiCompute" class="footnote" rel="footnote">14</a></sup>. Today’s GPUs reach several TFLOPs (Tera-FLOP) with a FLOP being a <em>“floating point operation”</em>. It has to be considered though that these can be 64-, 32-, 16-, 8-, or even just 4-bit operations! Nonetheless, 1 TFLOP = \(1\cdot10^{12}\) FLOP. The question is now how many operations can be done per second.
<button class="sidenote-button-left"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-left">
    The <strong>scaling hypothesis in AI</strong> posits that as the amount of data and computational resources available for training a machine learning model increases, the performance of the model also improves, often in a predictable manner. This hypothesis suggests that many AI tasks, particularly those related to deep learning, benefit from scaling up both the size of the dataset used for training and the computational power available for training the model.
    In practice, this means that larger neural networks trained on more extensive datasets tend to achieve better performance, such as higher accuracy or improved generalization, compared to smaller models trained on smaller datasets. The scaling hypothesis has been supported by empirical evidence in various domains, particularly in natural language processing, computer vision, and other areas where deep learning techniques are prevalent<sup id="fnref:ScalingHypo" role="doc-noteref"><a href="#fn:ScalingHypo" class="footnote" rel="footnote">15</a></sup><sup id="fnref:ScalingHypo3" role="doc-noteref"><a href="#fn:ScalingHypo3" class="footnote" rel="footnote">16</a></sup><sup id="fnref:ScalingHypo2" role="doc-noteref"><a href="#fn:ScalingHypo2" class="footnote" rel="footnote">17</a></sup>.
    <!--
    <style>
        img[alt=centerAI] { float: left; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    ![centerAI](/images/scaling_hypothesis.jpg) -->
</span></p>
<style>
    img[alt=centerAI] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px; }
    img:hover {
                            transform: scale(1.9);
                            box-shadow: 0px 1px 3px rgba(0, 0, 0, 0.25);
                            z-index: 10000;
}
</style>

<p><img src="/images/AI_compute.png" alt="centerAI" />
<span style="font-size: 14px;">
    The amount of compute required to train (a) neural network and do inference (b) after training.
    This figure was created by myself but the data was compiled by Jaime Sevilla et al.<sup id="fnref:AIDemand_Data_1" role="doc-noteref"><a href="#fn:AIDemand_Data_1" class="footnote" rel="footnote">18</a></sup><sup id="fnref:AIDemand_Data_2" role="doc-noteref"><a href="#fn:AIDemand_Data_2" class="footnote" rel="footnote">19</a></sup>.
</span></p>

<blockquote>
  <p>“Extrapolating the spectacular performance of GPT-3 into the future suggests that the answer to life, the universe and everything is just 4.398 trillion parameters.” Geoffrey Hinton<sup id="fnref:HintonScaling" role="doc-noteref"><a href="#fn:HintonScaling" class="footnote" rel="footnote">20</a></sup>.</p>
</blockquote>

<h3 id="21-training">2.1 Training</h3>
<p>Subplot (a) of the figure presented above shows the insane increase in training compute over time to create more and more capable AIs, rising orders of magnitude within years.</p>

<blockquote>
  <p>“[…] since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with <strong>a 3.4-month doubling time</strong> (by comparison, Moore’s Law had a 2-year doubling period). Since 2012, this metric has grown by more than <strong>300,000x</strong> (a 2-year doubling period would yield only a 7x increase).” This was previously observed by OpenAI<sup id="fnref:OpenAIComputePost" role="doc-noteref"><a href="#fn:OpenAIComputePost" class="footnote" rel="footnote">21</a></sup>.</p>
</blockquote>

<p>As we can see in the figure above, this is not only true for the largest AIs but for AI in general!</p>

<h3 id="22-inference">2.2 Inference</h3>
<p><button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
    <style>
        img[alt=NAchip] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    In fact, the <strong>North American AI chipset market</strong> is growing much faster for edge compute (which is essentially infernce) compared to cloud compute (mostly training) <sup id="fnref:NAChipMarket1" role="doc-noteref"><a href="#fn:NAChipMarket1" class="footnote" rel="footnote">22</a></sup>.
    <img src="/images/NA_AI_chipmarket.png" alt="NAchip" />
    In a live stream, George Hotz mockingly said:
    <em>“All those fucks are trying to make edge AI. […] Look! The AI inference market is bigger than the AI training market! So we should all go for the inference market! It’s way easier! […] Easier and bigger! […] There are a hundred little fucks who make inference chips that no one really wants and only one who is making training chips (NVIDIA).”</em> <sup id="fnref:GHotz_InferenceMarket" role="doc-noteref"><a href="#fn:GHotz_InferenceMarket" class="footnote" rel="footnote">23</a></sup>
    <style>
        img[alt=H100Buyers] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    <img src="/images/H100_buyers.png" alt="H100Buyers" />
    <em>Estimated 2023 H100 shipments by end customer as reported by Omida Research <sup id="fnref:OmidaH100Shipments" role="doc-noteref"><a href="#fn:OmidaH100Shipments" class="footnote" rel="footnote">24</a></sup>.</em>
</span>
Contrary to training, the compute for inference has hardly grown in comparison. That is because we want the inference to be fast and cheap in order for it to be practical. There can’t be a seconds of delay for the self-driving car. We don’t want our LLMs to spend hours computing an hour for our question and burn the energy of a small town for it. So with traditional hardware we are quite limited here and models are optimized to work within these boundaries. Imagine what we could do with better dedicated hardware though! Imagine LLMs would respond instantly and run locally on your phone. This will be necessary for robots especially. So while inference does not seem like a growth market at first glance, the demand is absolutely there.</p>

<h2 id="3-applications">3. Applications</h2>
<p>A figure from the paper showing how much it currently costs even to do just inference.
A. S. Luccioni et al. recently compared several machine learning tasks in terms of their energy consumption and nicely showed that image generation is orders of magnitudes more costly during inference compared to other ML tasks like image classification or text generation<sup id="fnref:AI_EnergyInference" role="doc-noteref"><a href="#fn:AI_EnergyInference" class="footnote" rel="footnote">25</a></sup>:</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>Inference Energy (kWh)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Text Classification</td>
      <td>0.002 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Extractive QA</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Masked Language Modeling</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Toke Classification</td>
      <td>0.004 \(\pm\) 0.002</td>
    </tr>
    <tr>
      <td>Image Classification</td>
      <td>0.007 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td>0.04 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>0.05 \(\pm\) 0.03</td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>0.49 \(\pm\) 0.01</td>
    </tr>
    <tr>
      <td>Image Captioning</td>
      <td>0.63 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Image Generation</td>
      <td>2.9 \(\pm\) 3.3</td>
    </tr>
  </tbody>
</table>

<h2 id="4-compute--energy--human-vs-gpt4">4. Compute / Energy:  Human vs. GPT4</h2>
<p>At this point, I want to highlight the title figure at the beginning of these notes showing the speed in GOP/s over power in W of GPUs, FPGAs, digital and mixed signal ASICs. More than any other figure here, I think it gives a sense of how the different hardware compares.</p>

<p>The sad truth as of now is that there seems to be a linear relationship between the amount of compute per second we can achieve and the energy required. To drive down the costs or AI developement and inference, we’d have to dramatically reduce the cost of energy, which would require an energy revolution in of itself, or rethink and rebuild our hardware on a much more fundamental level than is currently done (in the industry). That is precisely why I am writing these notes.</p>

<p>Comparisons to the human brain and how it developed are often made, especially now in context of LLMs and their extreme popularity and energy consumption because it requires historic energy consumption to train useful LLMs. Even if we compare all the energy consumed by a human throughout their entire life, the difference is vast. Some napkin math:</p>

<p>Assuming that the calorie intake is 2000 kcal / day \(\times\) 365 days where 1163 Wh = 1000 kcal<sup id="fnref:kcal_in_Watt" role="doc-noteref"><a href="#fn:kcal_in_Watt" class="footnote" rel="footnote">26</a></sup><sup id="fnref:kcal_in_Watt2" role="doc-noteref"><a href="#fn:kcal_in_Watt2" class="footnote" rel="footnote">27</a></sup>, that’s 730.000 kcal / year or \(8.5 \cdot 10^{-5}\) GWh/year per person. Now multiply this times the decades humans take to learn what LLMs know and we can maybe subtract or or two orders of magnitude in energy consumption. Still, in comparison, OpenAI used 50 GWh to train GPT4<sup id="fnref:GPT4_energy" role="doc-noteref"><a href="#fn:GPT4_energy" class="footnote" rel="footnote">28</a></sup>utilizing 25.000 NVIDIA A100 GPUs that run around 6.5 kW each.
It doesn’t take a math genius to see that there are orders of magnitude in difference. And as a result, the training cost for GPT4 was around $100.000.000 over a period of around 100 days (roughly $65M for the pre-training alone) as stated Sam Altman himself<sup id="fnref:GPT4_price" role="doc-noteref"><a href="#fn:GPT4_price" class="footnote" rel="footnote">29</a></sup><sup id="fnref:GPT4_facts" role="doc-noteref"><a href="#fn:GPT4_facts" class="footnote" rel="footnote">30</a></sup>.</p>

<h2 id="5-general-use-of-hardware">5. General Use of Hardware</h2>
<h3 id="51-gpus-nvidia-leads-the-way">5.1 GPUs: NVIDIA Leads the Way</h3>
<p>NVIDIA is the non-plus ultra for AI training at the largest scale. We see this reflected with commercial GPUs providing the fastest training. The drawback: These GPUs also consume by far the most power.</p>

<p>In NVIDIA’s most recent developer conference, Jensen Huang bragged about how hot these GPUs get. So hot, that they need to be liquid cooled (which poses new challenges in data centres). So hot, that the coolant coming out after just seconds could be used as a whirlpool, I remember him stating. Later on stage, he then continued talking about how NVIDIAs new accelerators are now much more energy efficient<sup id="fnref:NVIDIAGTC2024" role="doc-noteref"><a href="#fn:NVIDIAGTC2024" class="footnote" rel="footnote">31</a></sup>. It takes a special kind of genius to pull of such contradiction marketing. I am not even trying to be critical. It’s rather humorous!</p>

<p>Even with their incredible energy demands and the high training costs that follow, NVIDIAs glowing hot GPUs have a bright future.</p>

<p>That is because GPUs offer versatility and power for a wide range of applications. GPUs excel in scenarios where the workload is diverse or evolving, as they are capable of handling various computational tasks beyond AI, including graphics rendering, scientific simulations, and data analytics, all of which can compliment each other. They are <em>the</em> research plattform. Additionally ,they are simply extremely widespread. Nearly every computer has a GPU. Many have NVIDIA GPUs. And NVIDIA made sure to build the entire stack from the bottom up to the higher software layers to make it as easy for developers and scientists to get as much performance without needing to write much code.</p>

<h3 id="52-field-programmable-gate-arrays-fpgas-for-prototyping">5.2 Field-Programmable Gate Arrays (FPGAs) for Prototyping</h3>
<p>If we were to compete, lowering energy consumption is the angle of attack. FPGAs are, in part, competitive enough in terms of speed yet at similar power consumption and much, much worse usability. Nonetheless, some argue for a future of FPGAs:</p>

<blockquote>
  <p>“Due to their software-defined nature, FPGAs offer easier design flows and shorter time to market accelerators than an application-specific integrated circuit (ASIC).”<sup id="fnref:FPGACentres" role="doc-noteref"><a href="#fn:FPGACentres" class="footnote" rel="footnote">32</a></sup></p>
</blockquote>

<p>The advantage lies for tasks that require less compute but desire better optimization in speed and energy efficiency hence inference. And FPGAs are typically easier to bring to market than ASICs. So in scenarios where power consumption must be tightly controlled, such as in embedded systems or edge computing devices, FPGAs may provide a compelling solution. But compelling enough?</p>

<h3 id="53-application-specific-integrated-circuits-asics-for-on-the-edge-devices">5.3 Application-Specific Integrated Circuits (ASICs) for on the Edge Devices</h3>
<p>ASICs after all are by far superior for addressing energy concerns in AI hardware. Unlike general-purpose GPUs, ASICs are tailored specifically for certain tasks, maximizing efficiency and performance for those tasks while minimizing power consumption. On the other hand, ASICs entail higher initial development costs and longer time-to-market. Since progress in AI is driven primarily by new developments in software, developing ASICs that are general enough to keep up with that progress but specific enough to actually have an advantage over GPUs is tricky.</p>

<h3 id="54-tensor-processing-units-tpus-for-deep-learning-training">5.4 Tensor Processing Units (TPUs) for Deep Learning Training</h3>
<p>Developed by Google, TPUs are designed specifically for accelerating machine learning workloads, particularly those that involve neural network inference and training by focusing on matrix multiplication operations fundamental to neural network computations. And by doing so, TPUs are able to achieve remarkable speedups while consuming significantly less power compared to traditional GPU-based solutions. As I see it, they are simply not as widely available and their lack of generality compared to GPUs may make the development of AI applications a bit more difficult compared to GPUs once again.</p>

<h3 id="55-neural-processing-units-npus-for-inference">5.5 Neural Processing Units (NPUs) for Inference</h3>
<p>Similarly, NPUs represent a specialized class of AI hardware optimized specifically for neural network computations. They are designed to accelerate the execution of machine learning workloads but, and this is the crucial difference, <strong>with a focus on inference tasks</strong> commonly found in applications such as computer vision, natural language processing, and speech recognition. By incorporating dedicated hardware accelerators for key operations involved in neural network inference, such as convolutions and matrix multiplications, NPUs are able to achieve significant speedups while consuming less power. So the idea is similar to TPUs but with a focus more on inference specifically for edge devices and private computers.</p>

<h2 id="6-conclusion">6. Conclusion</h2>
<!--
+ Photonics is great at communication but has a lot of fundamental issues that keep it from being practical for computing in comparison electronics at least for now and likely the next 5-10 years. Integrating photonics as intra- and interconnects will be crucial for high-performance computing though. But it is not yet a bottleneck.

+ Thermodynamic computing, to me, seems mostly academic for now.

+ Quantum computing seems extremely promising but simply scaling this technology will take more decades as well.

+ Analog electronic computing on the other hand just works. The issue here is not the fundamental technology but rather our ability to design analog ICs quickly! It is my quess that analog electronics should also naturally play well with photonics. Very large scale integration (VLSI) is a resulting challenge. 
  >"Conventional analog designs require schematic-entry based specification and **a custom layout design that requires many iterations to meet design goals**."[^ADesign]

+ While I believe GPUs will continue to dominate the training market for the next 3+ years while digital ASICs, and to some degree FPGAs, will increasingly dominate the market for AI inference. But if we take a larger view of humanity and the required compute in the next decades and centuries, analog computing will have to become a dominating force. Not for general computing but for AI acceleration. And not just inference but, importantly, training as well.
-->

<p>What I and others ask is <strong>what is the computer of the future?</strong> I started writing this down because there are so many approaches to AI accelerators. Most quite traditional, simply designing digital ASICs. Some more exotic. Yet they are all extremely confident in that <em>their</em> approach is what the future AI super-computer will look like. That is why one of my I will now be more focused on the physics of computing. I’ll try and look at it from a first-principles standpoint, look at where our corrent technological limitations and lie to then give my personal estimate on what is the most sensible solution we should be working on.
<button class="sidenote-button-right"><span class="material-symbols-outlined">sticky_note_2</span></button>
<span class="sidenote-right">
    <strong>Thank you</strong> for reading these notes of mine as I tried to clear up my thinking on the subject. In the end, it lead me to the believe that it is rather pointless to try and directly compare all the available hardware. There are too many nuances and the technical details matter a lot. Yet, it is unclear how and what exactly matters and will be the most important metric to focus on for future computers. It seems to be quite clear to me that current computer designs are primarily driven by the econonmy and not a long-term focus and reasoning from first principles. So I hope to do a better job and illuminate this subject a bit more in a future post that takes a more physics oriented approach. Another lesson I took away for my own work and design process is that I need to iterate faster so I can prove my designs wrong faster as well. <br /> - Quentin
</span></p>

<h3 id="references">References</h3>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:MFigure" role="doc-endnote">
      <p>Adapted based on a figure originally linked to <a href="https://NICSEFC.EE.TSINGHUA.EDU.CN">https://NICSEFC.EE.TSINGHUA.EDU.CN</a>. Yet, I was not able to find it again as the link seemed to have been broken. <a href="#fnref:MFigure" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GoogleAGI" role="doc-endnote">
      <p>https://arxiv.org/pdf/2311.02462.pdf <a href="#fnref:GoogleAGI" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:RuchBioAI" role="doc-endnote">
      <p><a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://www.zurich.ibm.com/pdf/news/Towards_5D_Scaling.pdf">P. Ruch et al., <em>Toward five-dimensionalscaling: How densityimproves efficiency infuture computers</em>, IBM, 2011</a> <a href="#fnref:RuchBioAI" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ChipShortage" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/2020%E2%80%932023_global_chip_shortage <a href="#fnref:ChipShortage" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ChipShortage2" role="doc-endnote">
      <p>https://edition.cnn.com/2023/08/06/tech/ai-chips-supply-chain/index.html <a href="#fnref:ChipShortage2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:MuskVTransformers" role="doc-endnote">
      <p>Interview with Elon Musk on X.com: https://twitter.com/i/spaces/1YqJDgRydwaGV/peek Musk mentioned the various bottlenecks of AI in other recent interviews as well. <a href="#fnref:MuskVTransformers" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TransformerBottleneck" role="doc-endnote">
      <p>https://finance.yahoo.com/news/electrical-transformers-could-giant-bottleneck-220423599.html <a href="#fnref:TransformerBottleneck" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TFormer1" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/Data_center <a href="#fnref:TFormer1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TFormer2" role="doc-endnote">
      <p>https://www.olsun.com/power-integrator-data-center-2/ <a href="#fnref:TFormer2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:vonNeumannBottle" role="doc-endnote">
      <p>https://en.wikipedia.org/wiki/Von_Neumann_architecture <a href="#fnref:vonNeumannBottle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AKayHard" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Alan_Kay">Alan Kay</a>, <em><a href="https://www.folklore.org/Creative_Think.html">Creative Think</a></em>, 1982 <a href="#fnref:AKayHard" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ComputeNotMoney1" role="doc-endnote">
      <p><a href="https://x.com/Sentdex/status/1773358212403654860?s=20">Harrison Kinsley on X.com: https://x.com/Sentdex/status/1773358212403654860?s=20</a> <a href="#fnref:ComputeNotMoney1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:SamAltmanLex1" role="doc-endnote">
      <p>Sam Altman on the Lex Fridman podcast: <a href="https://lexfridman.com/sam-altman-2-transcript/">https://lexfridman.com/sam-altman-2-transcript/</a> <a href="#fnref:SamAltmanLex1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:WikiCompute" role="doc-endnote">
      <p><a href="Wikipedia: Floating Point Operations Per Second, https://de.wikipedia.org/wiki/Floating_Point_Operations_Per_Second">Wikipedia: Floating Point Operations Per Second, https://de.wikipedia.org/wiki/Floating_Point_Operations_Per_Second</a> <a href="#fnref:WikiCompute" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ScalingHypo" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/1712.00409.pdf?source=content_type%3Areact%7Cfirst_level_url%3Aarticle%7Csection%3Amain_content%7Cbutton%3Abody_link">J. Hestness et al., <em>DEEPLEARNING SCALING IS PREDICTABLE, EMPIRICALLY</em>, https://arxiv.org/pdf/1712.00409.pdf?source=content_type%3Areact%7Cfirst_level_url%3Aarticle%7Csection%3Amain_content%7Cbutton%3Abody_link, 2017</a> <a href="#fnref:ScalingHypo" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ScalingHypo3" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2001.08361.pdf">J. Kaplan et al., <em>Scaling Laws for Neural Language Models</em>, ArXiv, 2020: https://arxiv.org/pdf/2001.08361.pdf</a> <a href="#fnref:ScalingHypo3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ScalingHypo2" role="doc-endnote">
      <p>Gwern actually wrote about the scaling hypothesis on his blog as well: <a href="https://gwern.net/scaling-hypothesis">https://gwern.net/scaling-hypothesis</a> <a href="#fnref:ScalingHypo2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_1" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2202.05924.pdf">Sevilla et al., <em>Compute Trends Across Three Eras of Machine Learning</em>, ArXiv, 2022</a> <a href="#fnref:AIDemand_Data_1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_2" role="doc-endnote">
      <p><a href="https://epochai.org/blog/compute-trends">Epoch AI, <em>Compute Trends</em>, https://epochai.org/blog/compute-trends</a> <a href="#fnref:AIDemand_Data_2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:HintonScaling" role="doc-endnote">
      <p><a href="https://twitter.com/geoffreyhinton/status/1270814602931187715">Geoffrey Hinton on Twitter in 2020: https://twitter.com/geoffreyhinton/status/1270814602931187715</a> <a href="#fnref:HintonScaling" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:OpenAIComputePost" role="doc-endnote">
      <p>OpenAI, AI and compute: https://openai.com/research/ai-and-compute <a href="#fnref:OpenAIComputePost" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:NAChipMarket1" role="doc-endnote">
      <p>https://www.eetasia.com/ai-chip-market-to-reach-70b-by-2026/ <a href="#fnref:NAChipMarket1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GHotz_InferenceMarket" role="doc-endnote">
      <p>George Hotz, https://www.youtube.com/watch?v=iXupOjSZu1Y <a href="#fnref:GHotz_InferenceMarket" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:OmidaH100Shipments" role="doc-endnote">
      <p>Omida Research. (I was unable to find the original source though similar data can be found here: https://www.tomshardware.com/tech-industry/nvidia-ai-and-hpc-gpu-sales-reportedly-approached-half-a-million-units-in-q3-thanks-to-meta-facebook) <a href="#fnref:OmidaH100Shipments" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AI_EnergyInference" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2311.16863.pdf">Luccioni et al., <em>Power Hungry Processing: Watts Driving the Cost of AI Deployment?</em>, ArXiv, 2023</a> <a href="#fnref:AI_EnergyInference" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:kcal_in_Watt" role="doc-endnote">
      <p>Feel free to convert back and forth between kcal and Wh yourself at <a href="https://convertlive.com/u/convert/kilocalories-per-hour/to/watts#2000">https://convertlive.com/u/convert/kilocalories-per-hour/to/watts#2000</a>. <a href="#fnref:kcal_in_Watt" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:kcal_in_Watt2" role="doc-endnote">
      <p>https://de.wikipedia.org/wiki/Kalorie <a href="#fnref:kcal_in_Watt2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GPT4_energy" role="doc-endnote">
      <p>https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air <a href="#fnref:GPT4_energy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GPT4_price" role="doc-endnote">
      <p>Knight, Will. “OpenAI’s CEO Says the Age of Giant AI Models Is Already Over”. Wired. Archived from the original on April 18, 2023. Retrieved April 18, 2023 – via www.wired.com. https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/ <a href="#fnref:GPT4_price" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GPT4_facts" role="doc-endnote">
      <p>https://patmcguinness.substack.com/p/gpt-4-details-revealed <a href="#fnref:GPT4_facts" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:NVIDIAGTC2024" role="doc-endnote">
      <p><a href="https://www.youtube.com/watch?v=Y2F8yisiS6E&amp;list=PLZHnYvH1qtOYPPHRaHf9yPQkIcGpIUpdL">NVIDIA GTC March 2024 Keynote with Jensen Huang: https://www.youtube.com/watch?v=Y2F8yisiS6E&amp;list=PLZHnYvH1qtOYPPHRaHf9yPQkIcGpIUpdL</a> <a href="#fnref:NVIDIAGTC2024" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:FPGACentres" role="doc-endnote">
      <p>https://www.allaboutcircuits.com/news/shifting-to-a-field-programable-gate-array-data-center-future/ <a href="#fnref:FPGACentres" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quentin Wach</name></author><category term="science-engineering" /><category term="machine learning" /><category term="aritificial intelligence" /><category term="python" /><category term="analog computing" /><category term="electronics" /><category term="neural networks" /><category term="hardware design" /><category term="computer engineering" /><summary type="html"><![CDATA[An overview and commentary of commercial and academic AI and neuromorphic hardware. A rough comparison of ASICs, FPGAs, GPUs, and looking at compute requirements and energy consumption. I end this post with the realization that we need to take a first-principles physics approach to narrow down what future AI accelerator designs will have to look like because the market is complex and largely driven by momentum.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/AI_acc_comparison_QW_animated_WTtitle.gif" /><media:content medium="image" url="http://localhost:4000/images/AI_acc_comparison_QW_animated_WTtitle.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Analog Electronic Artificial Neural Network for Extreme Energy-Efficient Inference</title><link href="http://localhost:4000/science-engineering/2024/02/26/training-MNIST.html" rel="alternate" type="text/html" title="Analog Electronic Artificial Neural Network for Extreme Energy-Efficient Inference" /><published>2024-02-26T20:38:24+01:00</published><updated>2024-02-26T20:38:24+01:00</updated><id>http://localhost:4000/science-engineering/2024/02/26/training-MNIST</id><content type="html" xml:base="http://localhost:4000/science-engineering/2024/02/26/training-MNIST.html"><![CDATA[<ul>
  <li>Convert dense neural network in PyTorch to analog electric circuit based on op-amps.</li>
  <li>Implement non-linear activation functions as op-amp based neural network with linear activation functions.</li>
  <li>Discussion of the Landauer limit: In order to increase compute / energy beyond the limit, we must either make computation reversible or reduce the number of bits used. One way to reduce the number of bits is to simply go partially or even fully analog. Or we can stick to digital and make the logic reversible, meaning, no bits are wasted. Correct?</li>
</ul>

<!-- IMAGE: "/images/physics-processing-unit/quentinwach_A_modern_machine_brain_made_of_glass_metal_transist_c0e95e7c-154f-422d-b562-a1d5a2845fa7.png" -->
<!-- Challenges of Analog AI Accelerators -->
<!-- Read the <a href="/pdfs/Nanoscale_2024_2_13_SA.pdf"><button class="PDFButton">PDF</button></a>. -->
<p>Try the <a href="https://github.com/QuentinWach/QuentinWach/blob/master/README.md"><button class="PDFButton">Code on GitHub</button></a>.
<span class="sidenote-left">
This rather laborious post is meant to be somewhat of a starting point and reference for me to point to when friends with other interests and expertise want to know what I have been up to. At the same time, I am just starting to figure a lot of things out as well. I think this whole topic is not only awesome but important to at least think about.
</span>
<em>A deep artificial neural network (DNN) is trained and then reimplemented as an analog electronic circuit simulated with Python. The Python library is developed to converts the DNN design into an analog hardware design and simulate its operation. That is because an actual physical implementation is extremely costly due to the high cost of the individual components, notably the operational amplifiers, digital to analog converters and digital potentiometers. While a fully analog implementation of a neural network shows great promise in terms of energy efficiency and potentially speed, scaling this technology is not only challenging due to accumulating inaccuracies but also due to the high component costs. (10 TFLOPS at 4 W.)</em></p>
<div class="tag_list"> 
    <div class="tag">machine learning</div>
    <div class="tag">artificial intelligence</div>
    <div class="tag">python</div>
    <div class="tag">analog computing</div>
    <div class="tag">electronics</div>
    <div class="tag">neural networks</div>
    <div class="tag">hardware design</div>
    <div class="tag">computer engineering</div>
</div>
<style>
    img[alt=Board] {float: left; width: 100%; border-radius:5px; margin-right: 10px; margin-bottom: 30px; margin-top: 5px;}
</style>

<p><img src="/images/mem_analog_2024.png" alt="Board" /></p>

<p><span class="sidenote-left">
    <style>
        img[alt=me] {float: left; width: 100%; border-radius:5px; margin-right: 10px; margin-bottom: 30px; margin-top: 5px;}
    </style>
    <img src="/images/memes/floating_me.webp" alt="me" />
    Many thanks for reading!
</span></p>

<!--
Work through the following articles:
+ https://openaccess.thecvf.com/content/WACV2023W/WVAQ/papers/Ornhag_Accelerating_AI_Using_Next-Generation_Hardware_Possibilities_and_Challenges_With_Analog_WACVW_2023_paper.pdf
+ https://dl.acm.org/doi/pdf/10.1145/3453688.3461746?casa_token=4grhyhx2cCsAAAAA:Ho4pkCjh2oT-lwe99qegHoQLMOscAuKZ6KNCkipYP1CapWV4gvF2y8mEVAuZ7wG3zNNJA6J_v6ZFSA
+ https://www.sciencedirect.com/science/article/pii/S2095809921003349
+ https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864008&casa_token=ZKjaCrd4dj8AAAAA:4aMxRshEMCS2yJC1LBm9QFHq50gyIRrWsNaqg89gCeY8RD1qUCu1vxMCrvW3-FrThMoYq8jRB0c&tag=1
+ https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180545&casa_token=0XZ9QgOkFt4AAAAA:N7a31G75sAtUAwO2a6z3ID-R7GIz-DFKT15B1pv6XgM6aRNUR2caaT5TB_wVYJmIyHZKMeuK5f0
-->

<blockquote>
  <p>“People who are really serious about software should make their own hardware.” Alan Kay <sup id="fnref:AKayHard" role="doc-noteref"><a href="#fn:AKayHard" class="footnote" rel="footnote">1</a></sup></p>
</blockquote>

<p>The purpose of this project is to implement a neural network capable of classifying MNIST nunbers at competitive accuracies
as an analog electronic circuit. As such we will:</p>

<ol>
  <li>Build and train a dense neural network in Keras.</li>
  <li>Convert that model into a SPICE netlist with analog modules.</li>
  <li>Simulate the analog electronic ANN with PySPICE using NGSpice as backend.</li>
  <li>Discuss the results of the simulations compared to digital and other analog approaches for AI acceleration.</li>
  <li>Discuss issues, limitations, and desirable features.</li>
  <li>The general context of analog VLSI design, the next step of layout engineering using tools like magic, the Sky130 PDK and multi-wafer projects (MWPs) to produce an actual IC.</li>
</ol>

<h2 id="1-introduction">1 Introduction</h2>

<h3 id="compute-performance">Compute Performance</h3>
<blockquote>
  <p>“You’re trying to scale compute flops.
I’m trying to get rid of flops entirely.
We are not the same.” Guillaume Verdon (aka Beff Jezos) <sup id="fnref:TweetBeffJezos" role="doc-noteref"><a href="#fn:TweetBeffJezos" class="footnote" rel="footnote">2</a></sup></p>
</blockquote>

<p>Let us say that every mathematical operation (OP) is equivalent to a floating point operation (FLOP). Then we can simply look at the schematic of analog neural network and count the number of operators or rather elements that are operated on in one feedforward pass. We then measure or calculate how much time the network would require to do this calculation. This gives us:
<span class="sidenote-right">
    At least that is my intution but it might very well be that OPs and FLOPs really can’t be compared this directly.
</span></p>

\[\begin{equation}
    500.000~\text{FLOP} / 1~\mu\text{s}\\ = 500.000 \times 10^{6}~\text{FLOPS}\\= 5 \times 10^{11}~\text{FLOPS}\\ = 0.5 \times \text{TFLOPS} 
\end{equation}\]

<h3 id="energy-consumption">Energy Consumption</h3>
<p>At the same time, we can calculate how much \(\text{W}\) is consumed for that pass.</p>

<h3 id="compute-in-the-age-of-ai">Compute in the Age of AI</h3>

<h4 id="what-is-compute">What is Compute?</h4>
<blockquote>
  <p>“The base metric is becoming compute. Not money.” Harrison Kinsley (aka Sentdex) <sup id="fnref:ComputeNotMoney1" role="doc-noteref"><a href="#fn:ComputeNotMoney1" class="footnote" rel="footnote">3</a></sup></p>
</blockquote>

<p><em>“Compute”</em> or computing is simply a general term to describe how much computational power is available. So it is a measure of how powerful a computer is <sup id="fnref:WikiCompute" role="doc-noteref"><a href="#fn:WikiCompute" class="footnote" rel="footnote">4</a></sup>. Today’s GPUs reach several TFLOPs (Tera-FLOP) with a FLOP being a <em>“floating point operation”</em>. It has to be considered though that these can be 64-, 32-, 16-, 8-, or even just 4-bit operations! Nonetheless, 1 TFLOP = \(1\cdot10^{12}\) FLOP. The question is now how many operations can be done per second as well what KIND of operations. Multiplication? Addition? Some functional transformation?</p>
<style>
    img[alt=centerAI] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px;}
</style>

<p><img src="/images/AI_compute.png" alt="centerAI" />
<span style="font-size: 14px;">
    The amount of compute required to train (a) neural network and do inference (b) after training.
    This figure was created by myself but the data was compiled by Jaime Sevilla et al. <sup id="fnref:AIDemand_Data_1" role="doc-noteref"><a href="#fn:AIDemand_Data_1" class="footnote" rel="footnote">5</a></sup> <sup id="fnref:AIDemand_Data_2" role="doc-noteref"><a href="#fn:AIDemand_Data_2" class="footnote" rel="footnote">6</a></sup>.
</span></p>

<h4 id="the-arms-race-of-training-ais">The Arms Race of Training AIs</h4>

<h4 id="do-not-underestimate-the-market-and-importance-of-inference">Do Not Underestimate the Market and Importance of Inference</h4>
<p>Contrary to training, the compute for inference has hardly grown in comparison. That is because we want the inference to be fast and cheap in order for it to be practical. There can’t be a seconds of delay for the self-driving car. We don’t want our LLMs to spend hours computing an hour for our question and burn the energy of a small town for it. So with traditional hardware we are quite limited here and models are optimized to work within these boundaries. Imagine what we could do with better dedicated hardware though! Imagine LLMs would respond instantly and run locally on your phone. This will be necessary for robots especially. So while inference does not seem like a growth market at first glance, the demand is absolutely there.
<span class="sidenote-right">
    <style>
        img[alt=NAchip] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    In fact, the North American AI chipset market is growing much fast for edge compute (which is essentially infernce) compared to cloud compute (mostly training) <sup id="fnref:NAChipMarket1" role="doc-noteref"><a href="#fn:NAChipMarket1" class="footnote" rel="footnote">7</a></sup>.
    <img src="/images/NA_AI_chipmarket.png" alt="NAchip" />
    In a live stream, George Hotz mockingly said:
    <em>“All those fucks are trying to make edge AI. […] Look! The AI inference market is bigger than the AI training market! So we should all go for the inference market! It’s way easier! […] Easier and bigger! […] There are a hundred little fucks who make inference chips that no one really wants and only one who is making training chips (NVIDIA).”</em> <sup id="fnref:GHotz_InferenceMarket" role="doc-noteref"><a href="#fn:GHotz_InferenceMarket" class="footnote" rel="footnote">8</a></sup>
    <style>
        img[alt=H100Buyers] {float: right; width: 100%; border-radius:5px; margin-left: 10px; margin-bottom: 10px; margin-top: 10px;}
    </style>
    <img src="/images/H100_buyers.png" alt="H100Buyers" />
    <em>Estimated 2023 H100 shipments by end customer as reported by Omida Research <sup id="fnref:OmidaH100Shipments" role="doc-noteref"><a href="#fn:OmidaH100Shipments" class="footnote" rel="footnote">9</a></sup>.</em>
</span></p>

<h3 id="energy-consumption-1">Energy Consumption</h3>

<p>My own figure based on this data.</p>

<p>A figure from the paper showing how much it currently costs even to do just inference.
A. S. Luccioni et al. recently compared several machine learning tasks in terms of their energy consumption and nicely showed that image generation is orders of magnitudes more costly during inference compared to other ML tasks like image classification or text generation <sup id="fnref:AI_EnergyInference" role="doc-noteref"><a href="#fn:AI_EnergyInference" class="footnote" rel="footnote">10</a></sup>:</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>Inference Energy (kWh)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Text Classification</td>
      <td>0.002 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Extractive QA</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Masked Language Modeling</td>
      <td>0.003 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Toke Classification</td>
      <td>0.004 \(\pm\) 0.002</td>
    </tr>
    <tr>
      <td>Image Classification</td>
      <td>0.007 \(\pm\) 0.001</td>
    </tr>
    <tr>
      <td>Object Detection</td>
      <td>0.04 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Text Generation</td>
      <td>0.05 \(\pm\) 0.03</td>
    </tr>
    <tr>
      <td>Summarization</td>
      <td>0.49 \(\pm\) 0.01</td>
    </tr>
    <tr>
      <td>Image Captioning</td>
      <td>0.63 \(\pm\) 0.02</td>
    </tr>
    <tr>
      <td>Image Generation</td>
      <td>2.9 \(\pm\) 3.3</td>
    </tr>
  </tbody>
</table>

<p>10^23 FLOP for 10^6 USD thus 10^17 FLOP / USD = 10^5 TFLOP / USD or rather</p>

\[\$1 = 10^{17}~\text{FLOP} \\
\$1 = 100 \cdot \text{PFLOP}\]

<p>Figure comparing different hardware including the biological brain.</p>

<p><img src="/images/AI_acc_comparison_QW_animated.gif" alt="AIAccComp" /></p>

<p>AI Impacts writes</p>
<blockquote>
  <p>“In November 2017, we estimate the price for one GFLOPS to be between $0.03 and $3 for single or double precision performance, using GPUs (therefore excluding some applications). Amortized over three years, this is $1.1 x 10-5 -$1.1 x 10-7 /GFLOPShour” <sup id="fnref:AiImpacts" role="doc-noteref"><a href="#fn:AiImpacts" class="footnote" rel="footnote">11</a></sup>.</p>
</blockquote>

<h3 id="physics-imposed-limitations">Physics-Imposed Limitations</h3>
<p>What should be physically possible?</p>

<h3 id="accelerators">Accelerators</h3>

<p>As of now, GPUs are the way to go to train and run deep neural networks as they are by far the fastest, most common, available, and easy to use processors. All praise NVIDIA. Yet there are also ASICs like Google’s TPUs created specifically to improve the performance just for neural networks. Indeed, they require less power while being similarly fast. Still, the fastest training is, weirdly, achieved with GPUs. Then there are FPGAs that similar to ASICs achieve a lower power consumption at similar performance. Both ASICs and FPGAs are less general though. - More importantly, all available processors either consume a lot of power or have low performance. Ideally, we would like to get 100 OPS for less than 2 Watt, similarly to the brain.</p>

<p>I believe part of the solution are photonics as a way to speed up data transmission and bandwidth. But computation is extremely difficult and it will take much more time until photonic computers enter the competition in this particular field. We can use analog electronics though as well which, while less accurate, achieve incredible power-efficiency at high speeds! The key challenge here is thus dealing with noise and errors especially as we scale up the analog electronic circuit size.</p>

<p>I can’t seem to find much information on this topic online nor in books. So I will hack this together myself.</p>

<h3 id="digital-vs-analog">Digital vs Analog</h3>
<p>Most accelerators as of now still use digital signals to ensure accuracy and direct compatibility with existing technologies as the conversion from digital to analog and analog to digital is a science and art in itself. This leaves a wide opening to try and play with analog approaches though.</p>

<!-- <a href="https://everycircuit.com/circuit/5270808553062400">Analog Implementation of a Novel Resistive-Type Sigmoidal Neuron</a><br> -->
<iframe width="100%" height="500px" style="border-radius: 5px;" src="https://everycircuit.com/embed/5270808553062400" frameborder="0"></iframe>

<h2 id="analog-computing">Analog Computing</h2>
<h3 id="why-analog-circuits">Why Analog Circuits?</h3>
<p>Nowadays, most people don’t study analog computing.</p>

<p>Yet, we live in an analog world! And so, in order to do digital computing, we need to constantly convert from analog to digital and back, a very challenging task that some people make a living from just doing this alone! The reason digital computers are everywhere is because of their precision. They are reliable. They are therefore also easily scalable because new components do not introduce more and more errors. Hence why we now have over a hundred-billion transistors within our modern processors only a few cm in diameter. This is much more challenging to do with analog circuits. Note that quantum computing and photonic computing are predominately analog computing as well!</p>

<p>The great disadvantage with digital computing is that it requires a clock which can only operate at a very limited speed that hasn’t increased in recent decades and more importantly, digital circuits are extremely computation heavy and so consume a lot of energy.</p>

<p>Yet, for example, in analog computing using a memristors, matrix multiplication is almost trivial to do and done in a single shot at an extremely low energy consumption!</p>

<p><span class="sidenote-left">
    <strong>How to compare analog to digital compute?</strong> Analog computation has the advantage of not being limited to ones and zeroes as it computes directly with the floating values. This means we can also directly compare our compute with digital computers. This also highlights one of the main advantages of analog computing: 
    Lack of complexity! We do not need to <em>hundreds</em> of transistors to express a single number. It is directly encoded in our signals.
    <style>
        img[alt=sidenote] { float: right; width: 100%; border-radius:5px; margin-left: 10px;, margin-bottom: 10px; margin-top: 10px;}
    </style>
    <img src="/images/omni_man_analog_meme.png" alt="sidenote" />
</span></p>

<h3 id="operational-amplifier-as-a-black-box">Operational Amplifier as a Black Box</h3>
<p>The typical operational amplifier has two inputs \(V_{\text{in\pm}}\) and one output \(V_{\text{out}}\). Then</p>

\[V_{\text{out}} = A_0 \cdot (V_{\text{in+}} - V_{\text{in+}}).\]

<p>So what the op-amp cares about is the difference between the applied input voltages.
For the ideal case, we assume that the impedances are \(Z_{\text{in}} = \infty\) and \(Z_{\text{out}} = 0\) for the ideal op-amp. Besides this we want to run the operations at high speed and so we desire a bandwidth \(BW = \infty\) and a slew rate (the response rate) equaulling infinity. The output swing is supposed to be infinity as well. And finally, the gain. Sometimes it is desired to be infinity but it is actually depending on the definition. What do we mean by “ideal amplifier”? Here, we will wish it to be infinity as well, though. None of these goals/ideals can be achieved of course. But these are the goals.</p>

<!-- 
[WebLink about OpAmps](https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Instrumental_Analysis_(LibreTexts)/03%3A_Operational_Amplifiers_in_Chemical_Instrumentation_(TBD)/3.04%3A_Application_of_Operational_Amplifiers_to_Mathematical_Operations)

The increase in computation! (Also compare with the phoenix project paper!)
https://www.visualcapitalist.com/cp/charted-history-exponential-growth-in-ai-computation/
Better data:
https://arxiv.org/pdf/2202.05924.pdf
Costs of training: 
https://spectrum.ieee.org/state-of-ai-2023

https://ocw.mit.edu/courses/6-071j-introduction-to-electronics-signals-and-measurement-spring-2006/5bc88e4af14bc9c72e63f48cd2a44613_23_op_amps2.pdf

https://arxiv.org/pdf/2107.06283.pdf

https://www.analog.com/media/en/training-seminars/tutorials/MT-079.pdf

https://medium.com/@adi.fu7/ai-accelerators-part-i-intro-822c2cdb4ca4

https://www.youtube.com/watch?v=Qgjawf20v7Y

https://www.youtube.com/watch?v=kbVqTMy8HMg

https://www.youtube.com/watch?v=RTtpUi-gQOg

https://www.youtube.com/watch?v=VQoyypYTz2U

Neural Networks on FPGAs:
https://www.youtube.com/@marcowinzker3682

Still to watch:
https://www.youtube.com/watch?v=brhOo-_7NS4

https://ars.copernicus.org/articles/19/105/2021/

https://www.youtube.com/watch?v=VWn6Ixh2eDg

https://www.youtube.com/watch?v=VWn6Ixh2eDg

https://www.youtube.com/watch?v=GVsUOuSjvcg&t=898s

MYTHIC Analog computing interview:
https://www.youtube.com/watch?v=7PJJM__zbbE
Mythic is now shipping its analog AI inference processor. 25 TOPS at 3 watts!?  Crazy!
80.000 ADCs (analog to digitial converters)
High accuracy
Mass production hitting soon (if not already).
Last hurdle: Software usability.

Brain inspired Computing Nature Paper: FUNDAMENTAL!
https://www.nature.com/articles/s41467-019-12521-x

Von-Neumann Computing: Super important Nature paper, too!
https://www.nature.com/collections/dhdjceebhg

Still watch:
https://www.youtube.com/watch?v=EwueqdgIvq4

First full analog chip for machine learning:
https://www.aspinity.com/aml100
Yet it is mostly aimed at IOT applications!
How is it programmable? Field programmable? Is it an analog FPGA or smth?!

MUST READ FOR ELECTRONICS!
https://github.com/kitspace/awesome-electronics

Why are OPAMPS so expensive?
https://open.spotify.com/intl-de/track/164VgxTozx99XCinCB9ITR?si=d850164a39d54909


Not only are OPAMPS expensive. Digital potentiometers are even more expensive! But how do we load in our data then?

We can use the MCP4011-202E/MS digital potentiometer found on
https://www.arrow.com/en/products/mcp4011-202ems/microchip-technology?region=nac&utm_currency=USD. Yet, with around 800 input signals, this would cost us roughly 400€ given the price of 0,50€ per potentiometer.

Rather than building our own DAC (digital-to-analog converter) with so many outputs, we should attempt to find an already highly integrated pre-build DAC on the market. Typically, their output channel number is not greater than 32 or 64 though which means we'll need to multiplex.

4 channels

100 = 170€
340€ for 800 channels... This sucks...
https://www.digikey.de/en/products/detail/microchip-technology/MCP4728T-E-UN/5358293


Why are there so many fucking different OPAMS?
See the blog on my HHI laptop.

Why people don't think about analog computing. Yet it is everywhere!
https://electronics.stackexchange.com/questions/595199/what-is-the-actual-niche-for-operational-amplifiers-these-days


This paper shows how difficult it is to design, simulate, and actually build such an analog neural network as they only created a simulation as well! And they did so for a CMOS!
https://drive.google.com/file/d/1aGEucNV2uK2J1UHzV5xi8G5DK6MYNnQh/view
-->

<h3 id="analog-electronic-artificial-neuron">Analog Electronic Artificial Neuron</h3>
<p>It is actually extremely fascinating to think about spiking neural networks but given that these are as of now very uncommon
in state-of-the-art (SOTA) AIs, let’s focus on how we can build a simple perceptron-like artificial neural with analog electronics.</p>

<p>I wanted to keep this first experiment under a budget of 50€ because I am broke as hell. Most opamps sadly cost around $30ct though! But not all. We can use a LM258DRG4 by Texas Instruments which only costs $0.072 / piece (https://www.ti.com/product/LM258/part-details/LM258DRG4?HQS=ocb-tistore-invf-invftransact-invf-store-octopart-wwe found on https://octopart.com/search?category_id=4252&amp;start=790). If we have 300 opamps that’s less than 25€.</p>

<h2 id="dense-neural-network">Dense Neural Network</h2>
<p>We can recognize handwritten numbers using deep neural networks<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">12</a></sup> quite easily using convolutional layers<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">13</a></sup> but I will focus only on densily connected neurons for now. The dataset commonly used for this task is the famous <em>MNIST</em><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">14</a></sup> dataset. All of our code will be written in Python<sup id="fnref:Python" role="doc-noteref"><a href="#fn:Python" class="footnote" rel="footnote">15</a></sup>. Let’s begin with importing some libraries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">PyTorch</span> <span class="k">as</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">matplitlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>We then define the architecture of our network:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># input layer
</span>
<span class="c1"># hidden layers
</span>
<span class="c1"># output layer
</span>
</code></pre></div></div>

<h3 id="error-discussion">Error Discussion</h3>

<p>The first Perceptron was, in fact, implemented using a custom analog computer at the time[^PerceptronAnalog] (See Veritasium.).</p>

<blockquote>
  <p>“The perceptron was invented in 1943 by Warren McCulloch and Walter Pitts[^]. The first hardware implementation was Mark I Perceptron machine built in 1957 at the Cornell Aeronautical Laboratory by Frank Rosenblatt,[^] […]” (Wikipedia[^].)</p>
</blockquote>

<!-- 
<style>
    img[alt=Omniman] { float: right; width: 50%; border-radius:15px; padding-left: 10px;, padding-bottom: 10px; padding-top: 10px;}
</style>
![Omniman](/images/omni_man_analog_meme.png)
-->

<h3 id="analog-circuits">Analog Circuits</h3>

<p><strong>Signal Generators.</strong>
As it turns out, signal generators, implementing functions with analog circuitry, is a non-trivial task in its own right which has its own research field dedicated to it. But since our goal here is a <em>“simple”</em> ASIC with only a few non-linear functions, this should be manageable, right? <em>RIGHT?!</em></p>

<p><strong>Leaky ReLu.</strong></p>

<p><strong>Sigmoid.</strong></p>

<h3 id="creating-the-printed-circuit-board-pcb">Creating the Printed Circuit Board (PCB)</h3>
<p>In order to learn how to create custom PCBs, I have been watching and working through <a href="https://www.youtube.com/watch?v=aVUqaB0IMh4">this course by Philip Salmony (Phil’s Lab)</a>. The whole project is based on the STM32 microcontroller.</p>

<p>If you are already proficient in PCB design, skip this section because I will be going through some fairly basic stuff, I am sure. That’s because, at this point, wasn’t trained to build one and so I will also share here how I learned to design PCBs.</p>

<ul>
  <li>What is a micro-controller?</li>
  <li>What can the STM32 do / why did we choose it?</li>
  <li>Why do we add each of these capacitors?</li>
  <li>Why do we have a different source for analog voltage compared to the digital voltage?</li>
  <li>Why do we add a ferrite bead? To dissipate heat? Why is that necessary?</li>
  <li>Why is the internal crystal oscillator so bad that we have to add our own one? What does that mean?</li>
</ul>

<p>Integrated Development Environment for STM32:</p>

<h3 id="simulating-with-ltspice-and-python">Simulating with LTSpice and Python</h3>
<p>A quick search on YouTube doesn’t offer much when it comes to learning how to build analog electronic circuits. But I found this devlog: https://www.youtube.com/watch?v=lKwzFdG2–8&amp;list=PL_R4uxT5thflWVbSWtl-rx5_C_q0RxjyV&amp;index=5 which brought me to learn about</p>
<ul>
  <li>https://www.analog.com/en/resources/media-center/videos/series/ltspice-essentials-tutorial.html</li>
  <li>https://pypi.org/project/PyLTSpice/
software I will built on, to help me design the rather complex analog neural network and simulate it. If this does not work, I will just code it all from scratch or dive into IBMs analog neural network design &amp; simulation Python library [source?!].</li>
</ul>

<h3 id="visualizing-the-design">Visualizing the design</h3>
<p>To make this super awesome, let’s create a visualization of the design like here https://www.youtube.com/watch?v=0Fixr39X8S4 as well as the input and output of the network! 😁
For this, once again, I wrote a little library.</p>

<h3 id="whats-next">What’s Next?</h3>
<p>The great challenge of analog designs overall is managing noise. This project did not bother with that problem at all since it is a rather tiny prototype. But in order to scale it from a few hundred neurons to millions, we will need to come up with a way to either eliminate the noise problem or make it a feature just like it is in the human brain. Both are interesting directions though the latter would make more sense if we actually move away from artificial neural networks to spiking neural networks.</p>

<p>Due to the cost of the components, miniaturization will also be essential. But so far, so good!</p>

<div style="{background-color: 'crimson'; border-radius: 15px;}">
### Get One!
Thank you for reading 😁. (Or even just scrolling through all of this.) If you found this interesting, I encourage you to go and check out
    <div>
    <button href="" class="PDFButton">all the files on Github (free)</button>
    or
    <button href="" class="PDFButton">get a working and tested board ($49,99 + shipping)</button>
    </div>
</div>

<!--
### THE GREAT FUTURE CHALLENGE #1: Making it programmable. Thus allowing for upload of weights in a cheap way! (This problem was solved already by at least one company.)

### THE GREAT FUTURE CHALLENGE #2: Making it precise. Adding ADCs and DACs as well as a calibration loop to ensure that all the signals are as close to the desired, simulated signal as possible to minimize the added error.

### THE GREAT FUTURE CHALLENGE #3: Make a module that is scalable to the TFLOP regime at 3+GHz!

### THE GREAT FUTURE CHALLENGE #4: Allow for fast backpropagation and traininig.

### GFC #5: Create an API so that any Numpy or PyTorch-based ANN can be trained with this chip.

### GFC #6: Create a PCB that can be plugged into and controlled by a computer with a CPU and OS. How can we connect multiple chips and distribute training across them?
-->

<p>For a general AI training accelerator, I could make it support tinygrad first since it has a tiny set of operations:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Buffer                                                       # class of memory on this device
unary_op  (NOOP, EXP2, LOG2, CAST, SIN, SQRT)                # A -&gt; A
reduce_op (SUM, MAX)                                         # A -&gt; B (smaller size, B has 1 in shape)
binary_op (ADD, SUB, MUL, DIV, CMPEQ, MAX)                   # A + A -&gt; A (all the same size)
load_op   (EMPTY, CONST, FROM, CONTIGUOUS, CUSTOM)           # -&gt; A   (initialize data on device)
ternary_op (WHERE)                                           # A, A, A -&gt; A
</code></pre></div></div>
<p>and machine learning operations:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Relu, Log, Exp, Sin                            # unary ops
Sum, Max                                       # reduce ops (with axis argument)
Maximum, Add, Sub, Mul, Pow, Div, Equal        # binary ops (no broadcasting, use expand)
Expand, Reshape, Permute, Pad, Shrink, Flip    # movement ops
Where                                          # ternary ops
</code></pre></div></div>
<p>See the documentation <sup id="fnref:tinygrad" role="doc-noteref"><a href="#fn:tinygrad" class="footnote" rel="footnote">16</a></sup>.
I have no idea how to implment these in a hybrid computer right now. One step at a time!
(Also, how would we go about supporting PyTorch, Tensorflow, Keras, Numpy, …? Integrate some RISC-V?)</p>

<h3 id="references">References</h3>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:AKayHard" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Alan_Kay">Alan Kay</a>, <em><a href="https://www.folklore.org/Creative_Think.html">Creative Think</a></em>, 1982 <a href="#fnref:AKayHard" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:TweetBeffJezos" role="doc-endnote">
      <p>Guillaume Verdon, https://twitter.com/BasedBeffJezos/status/1775445241434431541 <a href="#fnref:TweetBeffJezos" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ComputeNotMoney1" role="doc-endnote">
      <p><a href="https://x.com/Sentdex/status/1773358212403654860?s=20">Harrison Kinsley on X.com: https://x.com/Sentdex/status/1773358212403654860?s=20</a> <a href="#fnref:ComputeNotMoney1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:WikiCompute" role="doc-endnote">
      <p>https://de.wikipedia.org/wiki/Floating_Point_Operations_Per_Second <a href="#fnref:WikiCompute" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_1" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2202.05924.pdf"><em>COMPUTE TRENDS ACROSS THREE ERAS OF MACHINE LEARNING</em>, ArXiv</a> <a href="#fnref:AIDemand_Data_1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AIDemand_Data_2" role="doc-endnote">
      <p>https://epochai.org/blog/compute-trends <a href="#fnref:AIDemand_Data_2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:NAChipMarket1" role="doc-endnote">
      <p>https://www.eetasia.com/ai-chip-market-to-reach-70b-by-2026/ <a href="#fnref:NAChipMarket1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GHotz_InferenceMarket" role="doc-endnote">
      <p>George Hotz, https://www.youtube.com/watch?v=iXupOjSZu1Y <a href="#fnref:GHotz_InferenceMarket" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:OmidaH100Shipments" role="doc-endnote">
      <p>Omida Research. (I was unable to find the original source though similar data can be found here: https://www.tomshardware.com/tech-industry/nvidia-ai-and-hpc-gpu-sales-reportedly-approached-half-a-million-units-in-q3-thanks-to-meta-facebook) <a href="#fnref:OmidaH100Shipments" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AI_EnergyInference" role="doc-endnote">
      <p>https://arxiv.org/pdf/2311.16863.pdf <a href="#fnref:AI_EnergyInference" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:AiImpacts" role="doc-endnote">
      <p>https://aiimpacts.org/current-flops-prices/ <a href="#fnref:AiImpacts" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:1" role="doc-endnote">
      <p>A <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>B <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>C <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:Python" role="doc-endnote">
      <p>Python <a href="#fnref:Python" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tinygrad" role="doc-endnote">
      <p>https://github.com/QuentinWach/tinygrad/blob/master/docs/adding_new_accelerators.md <a href="#fnref:tinygrad" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quentin Wach</name></author><category term="science-engineering" /><category term="machine learning" /><category term="aritificial intelligence" /><category term="python" /><category term="analog computing" /><category term="electronics" /><category term="neural networks" /><category term="hardware design" /><category term="computer engineering" /><summary type="html"><![CDATA[A dense artificial neural network is trained and then converted to an analog electronic circuit simulated with SPICE using Python. While a fully analog implementation of a neural network shows great promise in terms of energy efficiency and speed, a hybrid approach is necessary to make it programmable. This is an introduction. As such, I also discuss the advantages and problems of analog computing. (BM.1, the first version, is promising. BM.2 will be an 130 nm IC design. Work in progress.)]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/physics-processing-unit/machine_brain_2.png" /><media:content medium="image" url="http://localhost:4000/images/physics-processing-unit/machine_brain_2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Dubins Paths for Waveguide Routing</title><link href="http://localhost:4000/science-engineering/2024/02/15/dubins-paths-for-waveguide-routing.html" rel="alternate" type="text/html" title="Dubins Paths for Waveguide Routing" /><published>2024-02-15T20:38:24+01:00</published><updated>2024-02-15T20:38:24+01:00</updated><id>http://localhost:4000/science-engineering/2024/02/15/dubins-paths-for-waveguide-routing</id><content type="html" xml:base="http://localhost:4000/science-engineering/2024/02/15/dubins-paths-for-waveguide-routing.html"><![CDATA[<!--Read the <a href="/pdfs/Dubins Paths for Waveguide Routing - Quentin Wach - 2023.pdf"><button class="PDFButton">PDF</button></a>.-->
<p><span class="sidenote-left">
<em>(Updated: March 12, 2024.)</em> I deleted the majority of the code I presented when I posted this first as it was quite hacky and all the recources can be found in the links below. It should also be added that, while much of the photonics community does not seem to be aware of Dubins paths and instead talk about s-bends and so on, I believe libraries already implement similar methods and Dubins paths are more or less just a generalization that can make life easier at times. It sure did for me and my work.
</span></p>

<p><em>What does the perfect waveguide routing look like? Traditional waveguide interconnects lack efficiency and optimization, leading to suboptimal photonic integrated circuits (PICs). Addressing key concerns like minimizing internal and radiation losses, reducing cross-talk, and avoiding unnecessary crossings, I explored the concept of finding the shortest path between two points in a PIC given a minimum bending radius. Dubins paths, already well known in robotics and control theory, emerged as a simple and highly practical solution.</em></p>

<div class="tag_list"> 
    <div class="tag">photonic integrated circuits</div>
    <div class="tag">design automation</div>
    <div class="tag">robotics</div>
    <div class="tag">gds</div>
</div>

<h3 id="1-introduction">1 Introduction</h3>
<p>I’ll try to keep this short and not ramble too much. I just want to get this out there since I found it rather useful and can’t believe it isn’t yet standard.</p>

<p>When I started my work on layouts for photonic integrated circuit in September 2023, it became quickly obvious that the typical waveguide interconnects provided by libraries are not well behaved nor optimal as to improve the performance and compactness of the photonic integrated circuit. Most commonly, we have simple straights, circular arcs, all sorts of splines, as well as Euler-bends, in short: They are all simple analytical functions or splines that (try to) optimize themselves. All of these are either simple building blocks that leave most of the work to the designer who has to plan out and specify every route, or behave extremely poorly and lead to overly bendy results that may even cross each other.</p>

<p>So in my first two weeks there, I was thinking about what we have to consider. Waveguides have to fullfil multiple criteria. They need to be…</p>

<ul>
  <li>As short as possible as to minimize internal losses.</li>
  <li>As straight and continuos as possible as to minimize radiation losses.</li>
  <li>As far away from other waveguides and structures as to minimize cross-talk.</li>
  <li>Does not cross other waveguides unless close to a 90° angle if absolutely necessary.</li>
</ul>

<p>It became quickly obvious that trying to find an optimum for all these design parameters is impossible unless we either weigh their importance using a complicated metric we have to justify, or make much harder constraints. Since it is common to have a fixed minimum bending radius for waveguides, it makes sense to translate this here. I then boiled all of the considerations down to the single, I believe, most important question: <strong>“What is the shortest path between the pin or vector \(\vec{a}\) and pin or vector \(\vec{b}\) given a minimum bending radius?”</strong></p>

<p>As it turns out, the answer is rather simple and long established in the field of robotics / control theory. <strong>Dubins paths!</strong></p>

<h3 id="2-what-are-they">2 What are they?</h3>
<p>Dubins paths are named after Lester Dubins, who introduced them in the 1950s<sup id="fnref:DubinPaper" role="doc-noteref"><a href="#fn:DubinPaper" class="footnote" rel="footnote">1</a></sup>. They refer to the shortest paths that a vehicle can take from one point to another while constrained to move at a specific minimum turning radius. These paths are thus commonly studied in the field of motion planning for vehicles, particularly in robotics and aerospace engineering.</p>

<style>
    img[alt=dubin1] { float: right; width: 65%; padding: 10px; }
</style>

<p><img src="/images/dubin/wikipedia_3.png" alt="dubin1" /></p>

<p>These paths simply connect circular arcs with straights which leaves us with a couple of characteristic Dubins paths:</p>
<ul>
  <li><strong>LRL</strong> (Left-Right-Left)</li>
  <li><strong>RLR</strong> (Right-Left-Right)</li>
  <li><strong>LSL</strong> (Left-Straight-Left)</li>
  <li><strong>RSR</strong> (Right-Straight-Right)</li>
  <li><strong>LSR</strong> (Left-Straight-Right)</li>
  <li><strong>RSL</strong> (Right-Straight-Left)</li>
</ul>

<p>You can see three examples of the possible paths in the figure I adapted here<sup id="fnref:WikiDubin" role="doc-noteref"><a href="#fn:WikiDubin" class="footnote" rel="footnote">2</a></sup>.</p>

<h3 id="3-geometric-construction">3 Geometric Construction</h3>
<p>Dubins did indeed prove that these trajectories are the shortest paths mathematically. The geometric construction is quite intuitive and a nice toy problem to figure out on ones own but it can quickly explode into multiple pages of pen and paper calculations and diagrams which is why I am not going to go into it here. Instead, I refer to a great overview and explanation of the synthesis of Dubins paths given by David A. Anisi<sup id="fnref:DubinImplementPaper" role="doc-noteref"><a href="#fn:DubinImplementPaper" class="footnote" rel="footnote">3</a></sup>. A wonderful and in-depth guide is also given by Andy G<sup id="fnref:DubinGuide" role="doc-noteref"><a href="#fn:DubinGuide" class="footnote" rel="footnote">4</a></sup>.</p>

<h3 id="4-code">4 Code</h3>
<p>There is an abundance of implementations of Dubins paths available on the internet<sup id="fnref:code1" role="doc-noteref"><a href="#fn:code1" class="footnote" rel="footnote">5</a></sup>.</p>

<!--
What I present here is a hacked together version. First, it's useful to define a little helper to keep all the angles within the range of $$ [ 0, 2 \pi ) $$:
```python
import math as m

# normalizes an angle to the range [0, 2*pi)
def mod_to_pi(angle):
    return angle - 2.0*m.pi*m.floor(angle/2.0/m.pi)
```

Next, we actually find our Dubins paths using a planner function `general_planner()` which creates and compares the lengths of all six possible routes using `dubins_path_length()`. The data of the solution is then returned with `dubins_path()`.
```python
import nazca as nd
from nazca.interconnects import Interconnect

# Function to find the optimal Dubins path between two points
def general_planner(planner, alpha, beta, d):
    """
    Calculates the Dubins path between two points defined by their 
    angles and distance given a plan.

    Parameters:
        planner (str): Type of Dubins path. Can be one of 'LSL', 'RSR',
                        'LSR', 'RSL', 'RLR', or 'LRL'.
        alpha (float): Starting orientation angle in radians.
        beta (float): Ending orientation angle in radians.
        d (float): Distance between the two points.

    Returns:
        tuple or None: A tuple containing the Dubins path as a list of
        angles and distances, the mode of the planner, 
        and the cost of the path. 
        Returns None if the path is not feasible.
    """

    # Convert angles to sine and cosine
    sa = m.sin(alpha)
    sb = m.sin(beta)
    ca = m.cos(alpha)
    cb = m.cos(beta)
    c_ab = m.cos(alpha - beta)
    # Convert planner input to uppercase
    planner_uc = planner.upper()
    
    # Case LSL Dubins path
    if planner_uc == 'LSL':
        # Calculate intermediate values for LSL path
        tmp0 = d + sa - sb
        p_squared = 2 + (d * d) - (2 * c_ab) + (2 * d * (sa - sb))
        if p_squared < 0:  # Path not feasible
            return None
        tmp1 = m.atan2((cb - ca), tmp0)
        t = mod_to_pi(-alpha + tmp1)
        p = m.sqrt(p_squared)
        q = mod_to_pi(beta - tmp1)

    # Case RSR Dubins path (similar structure to LSL)
    elif planner_uc == 'RSR':
        # Calculate intermediate values for RSR path
        tmp0 = d - sa + sb
        p_squared = 2 + (d * d) - (2 * c_ab) + (2 * d * (sb - sa))
        if p_squared < 0:  # Path not feasible
            return None
        tmp1 = m.atan2((ca - cb), tmp0)
        t = mod_to_pi(alpha - tmp1)
        p = m.sqrt(p_squared)
        q = mod_to_pi(-beta + tmp1)

    # Case LSR Dubins path (similar structure to LSL)
    elif planner_uc == 'LSR':
        # Calculate intermediate values for LSR path
        p_squared = -2 + (d * d) + (2 * c_ab) + (2 * d * (sa + sb))
        if p_squared < 0:  # Path not feasible
            return None
        p = m.sqrt(p_squared)
        tmp2 = m.atan2((-ca - cb), (d + sa + sb)) - m.atan2(-2.0, p)
        t = mod_to_pi(-alpha + tmp2)
        q = mod_to_pi(-mod_to_pi(beta) + tmp2)

    # Case RSL Dubins path (similar structure to LSR)
    elif planner_uc == 'RSL':
        # Calculate intermediate values for RSL path
        p_squared = (d * d) - 2 + (2 * c_ab) - (2 * d * (sa + sb))
        if p_squared < 0:  # Path not feasible
            return None
        p = m.sqrt(p_squared)
        tmp2 = m.atan2((ca + cb), (d - sa - sb)) - m.atan2(2.0, p)
        t = mod_to_pi(alpha - tmp2)
        q = mod_to_pi(beta - tmp2)

    # Case RLR Dubins path (similar structure to LSL)
    elif planner_uc == 'RLR':
        # Calculate intermediate values for RLR path
        tmp_rlr = (6.0 - d * d + 2.0 * c_ab + 2.0 * d * (sa - sb)) / 8.0
        if abs(tmp_rlr) > 1.0:  # Path not feasible
            return None
        p = mod_to_pi(2 * m.pi - m.acos(tmp_rlr))
        t = mod_to_pi(alpha - m.atan2(ca - cb, d - sa + sb) + mod_to_pi(p / 2.0))
        q = mod_to_pi(alpha - beta - t + mod_to_pi(p))

    # Case LRL Dubins path (similar structure to RLR)
    elif planner_uc == 'LRL':
        # Calculate intermediate values for LRL path
        tmp_lrl = (6. - d * d + 2 * c_ab + 2 * d * (- sa + sb)) / 8.
        if abs(tmp_lrl) > 1:  # Path not feasible
            return None
        p = mod_to_pi(2 * m.pi - m.acos(tmp_lrl))
        t = mod_to_pi(-alpha - m.atan2(ca - cb, d + sa - sb) + p / 2.)
        q = mod_to_pi(mod_to_pi(beta) - alpha - t + mod_to_pi(p))

    else:
        print("The given plan ", planner, " is false.")  # Invalid planner input

    # Create the Dubins path as a list of angles and distances
    path = [t, p, q]

    # Adjust angles if planner segments are lowercase (for reverse motion)
    for i in [0, 2]:
        if planner[i].islower():
            path[i] = (2 * m.pi) - path[i]

    # Calculate the cost of the path (sum of absolute values of angles and distance)
    cost = sum(map(abs, path))

    return (path, mode, cost)

# Function to calculate the length of a Dubins path
def dubins_path_length(start, end, radius):
    # Unpack start and end configurations
    (sx, sy, syaw) = start
    (ex, ey, eyaw) = end
    
    # Convert angles to radians
    syaw = m.radians(syaw)
    eyaw = m.radians(eyaw)
    
    # Define the turning radius
    c = radius
    
    # Calculate differences in coordinates
    ex = ex - sx
    ey = ey - sy
    
    # Project end point onto start orientation
    lex = m.cos(syaw) * ex + m.sin(syaw) * ey
    ley = - m.sin(syaw) * ex + m.cos(syaw) * ey
    leyaw = eyaw - syaw
    
    # Calculate the total distance
    D = m.sqrt(lex ** 2.0 + ley ** 2.0)

    return D

# Finds the Dubins path between two points
def dubins_path(start, end, radius):

    # Calculate the length
    D = dubins_path_length(start, end, radius)
    d = D / radius

    # Define important angles
    theta = mod_to_pi(m.atan2(ley, lex))
    alpha = mod_to_pi(- theta)
    beta = mod_to_pi(leyaw - theta)

    # Iterate through all possible paths
    planners = ['LSL', 'RSR', 'LSR', 'RSL', 'RLR', 'LRL']
    bcost = float("inf")
    bt, bp, bq, bmode = None, None, None, None
    for planner in planners:

        # find the solution for the Dubins path
        solution = general_planner(planner, alpha, beta, d)

        if solution is None:
            continue

        # Collect the data from the solution
        (path, mode, cost) = solution
        (t, p, q) = path
        if bcost > cost:
            # Best cost
            bt, bp, bq, bmode = t, p, q, mode
            bcost = cost

    return (list(zip(bmode, [bt*c, bp*c, bq*c], [c] * 3)))
```
-->

<p>It doesn’t take much time to either do an implementation from scratch or adapt preexisting code. We can then wrap it all up into a single, simple to use function just like any other provided by the design library you might be using. In this case, I have been using the Nazca library<sup id="fnref:NazcaLib" role="doc-noteref"><a href="#fn:NazcaLib" class="footnote" rel="footnote">6</a></sup> which comes with several interconnects, including straights and circular arcs which are used often but very tedious and slow to work with alone. Using <code class="language-plaintext highlighter-rouge">dubin_p2p()</code>, we can simply define the start pin, the end pin, and our code will route a Dubins path between them using the straights and circular arcs provided by Nazca. Of course, this can be easily adapted to other tools like GDSFactory<sup id="fnref:GDSFactory" role="doc-noteref"><a href="#fn:GDSFactory" class="footnote" rel="footnote">7</a></sup>.</p>

<!--
```python
# Generate a Nazca cell for a given Dubins path solution
def gds_solution(xs, pin1, pin2, solution):
    """
    We draw the trajectory of a 
    given solution for a Dubins path between
    two points, here, the pins, to be rendered in .gds!
    """
    # Get the pin vectors from nazca pin1 and pin2
    start = pin1.xya()
    end = pin2.xya()

    # Change the angle for the second pin
    new_end = list(end)
    new_end[2] = new_end[2] - 180
    end = tuple(new_end)
        
    # Define the xs for the .gds file
    ic = Interconnect(xs=xs)

    # Create the cell object for the path
    with nd.Cell("dubins-path") as C:

        # Define important points
        radius = solution[0][2]
        current_position = start
        (sx, sy, syaw) = start
        (ex, ey, eyaw) = end
        ex = ex - sx
        ey = ey - sy

        # Draw the S, L, or R elements from the solution
        for (mode, length, radius) in solution:
            if mode == 'L':
                # Find the center of the circle
                center = (
                    current_position[0] 
                    + m.cos(m.radians(current_position[2] + 90)) * radius,

                    current_position[1]
                    + m.sin(m.radians(current_position[2] + 90)) * radius,
                )
                new_position = (
                    center[0] + m.cos(m.radians(current_position[2]
                    - 90 + (180 * length / (m.pi * radius)))) * radius,

                    center[1] + m.sin(m.radians(current_position[2]
                    - 90 + (180 * length / (m.pi * radius)))) * radius,

                    current_position[2] + (180 * length / (m.pi * radius))
                )
                arc_angle = (180 * length / (m.pi * radius))
                # Change this line if you want to use the function
                # in, for example, Matplotlib
                ic.bend(radius=radius, angle=arc_angle).put()
            elif mode == 'R':
                # Find the center of the circle
                center = (
                    current_position[0]
                    + m.cos(m.radians(current_position[2] - 90)) * radius,

                    current_position[1]
                    + m.sin(m.radians(current_position[2] - 90)) * radius,
                )
                new_position = (
                    center[0] + m.cos(m.radians(current_position[2]
                    + 90 - (180 * length / (m.pi * radius)))) * radius,

                    center[1] + m.sin(m.radians(current_position[2]
                    + 90 - (180 * length / (m.pi * radius)))) * radius,

                    current_position[2] - (180 * length / (m.pi * radius))
                )
                arc_angle = - (180 * length / (m.pi * radius))
                # Change this line if you want to use the function
                # in, for example, Matplotlib
                ic.bend(radius=radius, angle=arc_angle).put()
            elif mode == 'S':
                new_position = (
                    current_position[0]
                    + m.cos(m.radians(current_position[2])) * length,

                    current_position[1]
                    + m.sin(m.radians(current_position[2])) * length,

                    current_position[2],
                )
                xl = current_position[0] - new_position[0]
                yl = current_position[1] - new_position[1]
                l = m.sqrt(xl**2 + yl**2)
                # Change this line if you want to use the function
                # in, for example, Matplotlib
                ic.strt(length=l).put()
            else:
                print("Something ain't right, buddy.")
                
            current_position = new_position
    return C
-->

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#########################################################################
# Use this when designing your PIC with Nazca!
#########################################################################
</span>
<span class="c1"># Create Dubins path between two pins in Nazca
</span><span class="k">def</span> <span class="nf">dubin_p2p</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">pin1</span><span class="p">,</span> <span class="n">pin2</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Finds and creates the shortest possible path between two vectors
    (pin1 and pin2) with a minimum bending radius,
    a so called </span><span class="sh">"</span><span class="s">Dubins path</span><span class="sh">"</span><span class="s">. This Dubins path is made of two
    circular bends and a straight waveguide.
    Returns a cell containing these waveguides.

    IMPORTANT
    =========
    In this version, you NEED to specify to put the path at the starting
    pin so if pin1=IO.pin[</span><span class="sh">"</span><span class="s">a0</span><span class="sh">"</span><span class="s">] you must add .put(IO.pin[</span><span class="sh">"</span><span class="s">a0</span><span class="sh">"</span><span class="s">]).
    Else, the Dubins path will be generated correctly
    but possibly at the wrong position. 
    
    PARAMETERS
    ==========
    xs:     Crosssection parameters.
    pin1:   The start pin to which the Dubins path attaches.
    pin2:   The end pin to where the Dubins path ends.
    radius: The minimum bending radius for the Dubins paths.
    width:  The width of the waveguides dubin_p2p creates.
    </span><span class="sh">"""</span>
    <span class="c1"># Get the pin vectors from pin1 and pin2
</span>    <span class="n">START</span> <span class="o">=</span> <span class="n">pin1</span><span class="p">.</span><span class="nf">xya</span><span class="p">()</span>
    <span class="n">END</span> <span class="o">=</span> <span class="n">pin2</span><span class="p">.</span><span class="nf">xya</span><span class="p">()</span>

    <span class="c1"># Change the angle for the second pin
</span>    <span class="n">new_end</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">END</span><span class="p">)</span>
    <span class="n">new_end</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_end</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">180</span>
    <span class="n">END</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">new_end</span><span class="p">)</span>

    <span class="c1"># Find the Dubins path between pin1 and pin2
</span>    <span class="n">path</span> <span class="o">=</span> <span class="nf">dubins_path</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">START</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">END</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="n">radius</span><span class="p">)</span>

    <span class="c1"># Create the Dubins path with nazca using bends and straights
</span>    <span class="k">return</span> <span class="nf">gds_solution</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">pin1</span><span class="p">,</span> <span class="n">pin2</span><span class="p">,</span> <span class="n">solution</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>

</code></pre></div></div>

<h3 id="5-conclusion">5 Conclusion</h3>
<p>In the figure below, a comparison between a dense array of Dubins paths and arrays using Nazcas s-bends and cobra splines is made:</p>

<p><img src="/images/dubin/DubinAdvantage.PNG" alt="sleep_figure_1" /></p>

<p>As one can see, not only do other interconnects lead to over-bending of the waveguides and thus a longer path and greater losses, they are also less reliable, predictable, <strong>often break the design rules to not violate the minimum bending radius</strong> as is indeed the case here. They <strong>even intersect each other!</strong> Meanwhile, the Dubins paths behave extremely predictably. They clearly show the shortest path without unnecessary bends and they do not intersect each other which allows for much denser layouts than would be possible with the other interconnects. What I realized only after this was that there are indeed interconnects provided by Nazca which are essentially Dubins paths like <code class="language-plaintext highlighter-rouge">bend_strt_bend_p2p()</code> though not as general.</p>

<p>Also, there is a list of improvements one may make based on this. For one, the curvature of Dubins paths are not smooth which may lead to higher radiation losses. The hours of headaches I personally avoided just by using Dubins paths are insane. It is also simply much more enjoyable to use.</p>

<p>That’s my little tip for those working on photonic integrated circuit layouts. I hope it helps!</p>

<!--### BibTeX
```bibtex
@article{QWachDubin2024,
    author = {Quentin Wach},
    title = {Dubins Paths for Waveguide Routing},
    year = {2024}
```
} -->

<h3 id="references">References</h3>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:DubinPaper" role="doc-endnote">
      <p><a href="https://doi.org/10.2307/2372560">Dubins, L. E., <em>“On Curves of Minimal Length with a Constraint on Average Curvature, and with Prescribed Initial and Terminal Positions and Tangents”</em>. American Journal of Mathematics. 79 (3): 497–516, 1957</a> <a href="#fnref:DubinPaper" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:WikiDubin" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Dubins_path">Wikipedia: Dubins Paths (Accessed: Feb 20, 2024)</a> <a href="#fnref:WikiDubin" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:DubinImplementPaper" role="doc-endnote">
      <p><a href="https://people.kth.se/~anisi/articles/foi-r-0961-se.pdf">David A. Anisi, <em>“Optimal Motion Control of a Ground Vehicle”</em>, 2003</a> <a href="#fnref:DubinImplementPaper" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:DubinGuide" role="doc-endnote">
      <p><a href="https://gieseanw.wordpress.com/2012/10/21/a-comprehensive-step-by-step-tutorial-to-computing-dubins-paths/">Andy G, <em>“A Comprehensive Step by Step Tutorial to Computing Dubins Paths”</em></a> <a href="#fnref:DubinGuide" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:code1" role="doc-endnote">
      <p><a href="https://atsushisakai.github.io/PythonRobotics/modules/path_planning/dubins_path/dubins_path.html">Atsushi Sakai, <em>“Python Robotics: Dubins Path Planning”</em>, GitHub. (Accessed: Feb 20, 2024)</a> <a href="#fnref:code1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:NazcaLib" role="doc-endnote">
      <p><a href="https://nazca-design.org/">Nazca Design: Photonic IC Design Framework</a> <a href="#fnref:NazcaLib" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:GDSFactory" role="doc-endnote">
      <p><a href="https://github.com/gdsfactory/gdsfactory">GDSFactory, GitHub</a> <a href="#fnref:GDSFactory" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Quentin Wach</name></author><category term="science-engineering" /><category term="physics" /><category term="photonics" /><category term="design" /><category term="routing" /><category term="gds" /><summary type="html"><![CDATA[What does the perfect waveguide routing look like? Dubins paths, already well known in robotics and control theory, turn out to be a simple and highly practical solution. This post notes a few useful recources and explains what the advantages of Dubins paths are compared to other waveguide interconnects that are often used.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/dubin/wikipedia_4.png" /><media:content medium="image" url="http://localhost:4000/images/dubin/wikipedia_4.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">From Art to Science</title><link href="http://localhost:4000/personal/2024/02/10/Who-am-I.html" rel="alternate" type="text/html" title="From Art to Science" /><published>2024-02-10T20:38:24+01:00</published><updated>2024-02-10T20:38:24+01:00</updated><id>http://localhost:4000/personal/2024/02/10/Who-am-I</id><content type="html" xml:base="http://localhost:4000/personal/2024/02/10/Who-am-I.html"><![CDATA[<!-- You might be wondering: "Who the hell is this guy?" And since I keep retelling certain stories over and over as an answer, here is a common one:
-->
<h3 id="among-artists">Among Artists</h3>
<p>I come from a family of designers and artists. It was pretty much decided by every person I knew that given my seemingly natural talents given in these areas as well, I should become an artist, designer, or musician, or just all of the above. (You can see me doodling as a kid as well as a couple of sketches I did here.)</p>

<p>After all, given the circumstances, I had all the tools, experience, connection, and motivation to quickly acquire all sorts of creative abilities. But drawing, painting, sculpting, … it was all a way for me to not just capture the beauty of the world but to understand it.</p>

<style>
    img[alt=Sketch2] { float: right; width: 50%; padding: 10px;}
</style>

<p><img src="/images/Sketch2.png" alt="Sketch2" /></p>

<p>Of the art books we had, a collection of Leonardo Da Vinci’s works stood out to me and I related to this mindset from a young age as art was not meant to solely entertain but it was a tool to capture the beauty of the world, understanding it, recreating it.</p>

<blockquote>
  <p>“If you can create it, you understand it.” like Feynman said.</p>
</blockquote>

<p>Naturally, I liked to read every popular science and textbook I could find.</p>

<h3 id="new-age-of-ai">New Age of AI</h3>
<p>When I was 16, the new age of AI had just begun. It was clear to me that this was the future and like many curious, I had always been fascinated with science and the brain as well. A lot of the problems I had previously broken my head over were suddenly solved! Now everything seemed possible within my lifetime.</p>

<p>I taught myself programming and started recreating AI publications I found on the Arxiv, including generative AI like Generative Adversarial Networks and its many variations that came out during those years. (I also ruined a bunch of hard drives and computers in the process.) If I had any ambition to become a great artist, this was the last bullet that killed it. I can do meaningful scientific research <em>and</em> push the boundaries of art and technology? Call me in.</p>

<h3 id="studying-physics-in-berlin">Studying Physics in Berlin</h3>
<p>Coding a lot, I realized I needed a more formal education in mathematics. At the same time, I was frustrated with my own understanding of the universe to such a painful degree that I knew I had to go to university and learn physics. I was helped with this decision knowing that many trained physicists were indeed AI researchers. So at 18, I moved across the country to the big most international city Germany had to offer: Berlin. Now was the time to actually become fluent in English as well, a subject I had no interest until I realized I wanted/needed to leave the country eventually.</p>

<p>Now that I had my first research experience and feel confident working on a wide variety of science and engineering problems, I am about to make such a jump again.</p>

<h3 id="conclusion">Conclusion</h3>
<p>During all of this, people looked at me with big eyes not understanding how I could throw all my <em>talents</em> away. Many were convinced I would give up quickly again as well. But eventually, as I stuck with physics, my image changed in the eyes of others and I became the physics guy. Suddenly, people told me how they hated physics and that they, contrary to me, had a more “creative” brain. Left brain vs. right brain… We know the myth. But I hope my story here dispelled it to some degree.</p>

<p>I seemingly chose to throw a lot of potential away to follow a more difficult path. But I saw neither fulfillment nor utility in a career of art, design, music, or entertainment.</p>

<p>A difficult life full of challenges and meaning is better than an easy one without.</p>]]></content><author><name>Quentin Wach</name></author><category term="personal" /><summary type="html"><![CDATA[This is an introduction to where I come from. I was born into a family of artists and designers and developed these talents rapidly making money with paintings and music from an early age. Then I chose to study physics. Because a meaningful life full of challenges is better than an easy life without.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/Sketch3.png" /><media:content medium="image" url="http://localhost:4000/images/Sketch3.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PDFtoCards: Flashcards for Everything</title><link href="http://localhost:4000/science-engineering/2023/06/06/PDFtoCards.html" rel="alternate" type="text/html" title="PDFtoCards: Flashcards for Everything" /><published>2023-06-06T21:38:24+02:00</published><updated>2023-06-06T21:38:24+02:00</updated><id>http://localhost:4000/science-engineering/2023/06/06/PDFtoCards</id><content type="html" xml:base="http://localhost:4000/science-engineering/2023/06/06/PDFtoCards.html"><![CDATA[]]></content><author><name>Quentin Wach</name></author><category term="science-engineering" /><category term="studying" /><category term="university" /><category term="ChatGPT" /><summary type="html"><![CDATA[Convert any book or article in PDF form to a deck of flashcards! Recognizes and understands text, equations, images, and (some) handwriting. Summarizes the PDF as questions and answers systematically. ChatGPT is used for understanding the content and MathPix to extract equations and images.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/PDFtoCards_1x1_v2.png" /><media:content medium="image" url="http://localhost:4000/images/PDFtoCards_1x1_v2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">THz Response of Charge Carriers in Nanoparticles</title><link href="http://localhost:4000/science-engineering/2023/01/01/first-paper.html" rel="alternate" type="text/html" title="THz Response of Charge Carriers in Nanoparticles" /><published>2023-01-01T20:38:24+01:00</published><updated>2023-01-01T20:38:24+01:00</updated><id>http://localhost:4000/science-engineering/2023/01/01/first-paper</id><content type="html" xml:base="http://localhost:4000/science-engineering/2023/01/01/first-paper.html"><![CDATA[]]></content><author><name>Michael T. Quick</name></author><category term="science-engineering" /><category term="physics" /><category term="optics" /><category term="THz spectroscopy" /><category term="thesis" /><category term="density matrix" /><category term="quantum mechanics" /><category term="semiconductors" /><category term="nanoscience" /><summary type="html"><![CDATA[Presenting a new quantum mechanical theory that models the THz mobility of charge carriers in low-dimensional semiconductors, revealing nonlinearities even at low field-strengths as well as a quantum mechanical equilibration current that counteracts the mobility at low frequencies.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/a.jpg" /><media:content medium="image" url="http://localhost:4000/images/a.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Essays: The Blank Spot</title><link href="http://localhost:4000/personal/2021/01/01/the-blank-spot.html" rel="alternate" type="text/html" title="Essays: The Blank Spot" /><published>2021-01-01T00:00:00+01:00</published><updated>2021-01-01T00:00:00+01:00</updated><id>http://localhost:4000/personal/2021/01/01/the-blank-spot</id><content type="html" xml:base="http://localhost:4000/personal/2021/01/01/the-blank-spot.html"><![CDATA[]]></content><author><name>[&quot;Quentin Wach&quot;]</name></author><category term="personal" /><category term="essay" /><category term="writing" /><summary type="html"><![CDATA[Being done with lab work and not busy with research either, I spent some time writing short essays on various subjects aiming to ask questions like: _'Is science a young man's game?'_, _'What is the relationship between genius, creativity, and insanity?'_, and _'How can we make academic articles freely accessible?'_]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/blank_spot_1x1V2.png" /><media:content medium="image" url="http://localhost:4000/images/blank_spot_1x1V2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Simulating &amp;amp; Animating Science with Python</title><link href="http://localhost:4000/science-engineering/2020/01/01/animating-science.html" rel="alternate" type="text/html" title="Simulating &amp;amp; Animating Science with Python" /><published>2020-01-01T00:00:00+01:00</published><updated>2020-01-01T00:00:00+01:00</updated><id>http://localhost:4000/science-engineering/2020/01/01/animating-science</id><content type="html" xml:base="http://localhost:4000/science-engineering/2020/01/01/animating-science.html"><![CDATA[]]></content><author><name>[&quot;Quentin Wach&quot;]</name></author><category term="science-engineering" /><category term="python" /><category term="animation" /><category term="matplotlib" /><category term="scipy" /><summary type="html"><![CDATA[A free collection of tutorials and simulation examples that aim to showcase and explain the animation capabilites one has using Python. This includes chemical reaction diffusion, the viral spread in pandemics, fractals, chaotic pendulums with springs, Conway's game of life, quantum states of hydrogen, galaxy collisions, and much more.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/science%20animations/happy4_c2.gif" /><media:content medium="image" url="http://localhost:4000/images/science%20animations/happy4_c2.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>